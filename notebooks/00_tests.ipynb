{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Tests\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_bml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spotRiver.evaluation.eval_bml import eval_bml_horizon, eval_bml_landmark, eval_bml_window, eval_oml_horizon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "metric=mean_absolute_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "df_eval , df_true = eval_bml_horizon(model , train , test , target_column, horizon, metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_bml_landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spotRiver.evaluation.eval_bml import eval_bml_landmark\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "df_eval , df_true = eval_bml_landmark(model , train , test , target_column, horizon, metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_bml_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spotRiver.evaluation.eval_bml import eval_bml_window\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "df_eval , df_true = eval_bml_window(model , train , test , target_column, horizon, metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_oml_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model\n",
    "from river import preprocessing,datasets\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "print(f\"train: {train.shape}, test: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, metric=metric) \n",
    "print(df_eval)\n",
    "print(df_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model, datasets, preprocessing\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "dataset = datasets.TrumpApproval()\n",
    "target_column = \"Approve\"\n",
    "df = convert_to_df(dataset, target_column)\n",
    "train = df[:500]\n",
    "test = df[500:]\n",
    "horizon = 1\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\n",
    "plot_bml_oml_horizon_metrics(df_eval, metric=metric)\n",
    "plot_bml_oml_horizon_predictions(df_preds, target_column=target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_oml_iter_progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from spotRiver.evaluation.eval_oml import eval_oml_iter_progressive, plot_oml_iter_progressive\n",
    "from river import metrics as river_metrics\n",
    "from river import tree as river_tree\n",
    "from river import preprocessing as river_preprocessing\n",
    "dataset = datasets.TrumpApproval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  (river_preprocessing.StandardScaler() | river_tree.HoeffdingAdaptiveTreeRegressor(seed=1))\n",
    "\n",
    "res_num = eval_oml_iter_progressive(\n",
    "    dataset = list(dataset),\n",
    "    step = 1,\n",
    "    metric = river_metrics.MAE(),\n",
    "    models =\n",
    "    {\n",
    "        \"HATR\": model,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_oml_iter_progressive(res_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(100) // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(100) % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(100) // 7\n",
    "# Remove the last two entries\n",
    "rem = 100 % 7\n",
    "arr = arr[:-rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model, datasets\n",
    "from river import preprocessing\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "dataset = datasets.TrumpApproval()\n",
    "target_column = \"Approval\"\n",
    "df = convert_to_df(dataset, target_column)\n",
    "train = df[:500]\n",
    "test = df[500:]\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "\n",
    "horizon = 10\n",
    "oml_grace_period = 10\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, oml_grace_period, metric=metric)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean of the MAEs of the predicted values and ignore the NaN values\n",
    "df_eval = df_eval.dropna()\n",
    "y = df_eval[\"Metric\"].mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1_000_000 / (7*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "250000/(7*24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, memory: float, r_time: float, metric) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance based on its predictions and ground truth values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth values as a numpy array.\n",
    "    y_pred: Predicted values as a numpy array.\n",
    "    memory: Memory usage in MB.\n",
    "    r_time: Computation time in seconds.\n",
    "    metric: A function that takes in two arguments (y_true and y_pred) and returns a score.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the evaluation results including the metric score, memory usage and computation time.\n",
    "\n",
    "    Example\n",
    "    ------\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "        import numpy as np\n",
    "        y_true = np.array([1.0, 2.0, 3.0])\n",
    "        y_pred = np.array([1.1, 2.1, 2.9])\n",
    "        memory = 100\n",
    "        r_time = 0.5\n",
    "        result = evaluate_model(y_true=y_true,\n",
    "                                y_pred=y_pred,\n",
    "                                memory=memory,\n",
    "                                r_time=r_time,\n",
    "                                metric=mean_squared_error)\n",
    "        print(result)\n",
    "\n",
    "        # Output:\n",
    "        # {'Metric': 0.00666666666666671, 'Memory (MB)': 100, 'CompTime (s)': 0.5}\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"y_true and y_pred must have the same size\")\n",
    "    if (len(y_true) == 0) or (len(y_pred) == 0):\n",
    "        res_dict = {\n",
    "            \"Metric\": None,\n",
    "            \"Memory (MB)\": memory,\n",
    "            \"CompTime (s)\": r_time,\n",
    "        }\n",
    "        return res_dict\n",
    "    score = metric(y_true, y_pred)\n",
    "    res_dict = {\"Metric\": score, \"Memory (MB)\": memory, \"CompTime (s)\": r_time}\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "y_true = np.array([1.0, 2.0, 3.0])\n",
    "y_pred = np.array([1.1, 2.1, 2.9])\n",
    "memory = 100\n",
    "r_time = 0.5\n",
    "result = evaluate_model(y_true=y_true,\n",
    "                        y_pred=y_pred,\n",
    "                        memory=memory,\n",
    "                        r_time=r_time,\n",
    "                        metric=mean_squared_error)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expect a ValueError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_model():\n",
    "    y_true = np.array([1.0, 2.0])\n",
    "    y_pred = np.array([1.1, 2.1, 2.9])\n",
    "    memory = 100\n",
    "    r_time = 0.5\n",
    "\n",
    "    try:\n",
    "        result = evaluate_model(y_true=y_true,\n",
    "                                y_pred=y_pred,\n",
    "                                memory=memory,\n",
    "                                r_time=r_time,\n",
    "                                metric=mean_squared_error)\n",
    "    except ValueError as e:\n",
    "        assert str(e) == \"y_true and y_pred must have the same size\"\n",
    "\n",
    "test_evaluate_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class ResourceMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import tracemalloc\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "class ResourceMonitorError(Exception):\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class ResourceUsage:\n",
    "    name: Optional[str]  # Description of Usage\n",
    "    r_time: float  # Measured in seconds\n",
    "    memory: float  # Measured in bytes\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.name is None:\n",
    "            res = [f\"Resource usage for {self.name}:\"]\n",
    "        else:\n",
    "            res = [\"Resource usage:\"]\n",
    "        res.append(f\"  Time [s]: {self.r_time}\")\n",
    "        res.append(f\"  Memory [b]: {self.memory}\")\n",
    "        return \"\\n\".join(res)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        self.name = name\n",
    "        self.r_time = None\n",
    "        self.memory = None\n",
    "        self.current_memory = None\n",
    "        self.peak_memory = None\n",
    "        self._start = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        if tracemalloc.is_tracing():\n",
    "            raise ResourceMonitorError(\"Already tracing memory usage!\")\n",
    "        tracemalloc.start()\n",
    "        tracemalloc.reset_peak()\n",
    "        self._start = time.perf_counter_ns()\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.r_time = (time.perf_counter_ns() - self._start) / 1.0e9\n",
    "        _, peak = tracemalloc.get_traced_memory()\n",
    "        self.memory = peak / (1024 * 1024)\n",
    "        tracemalloc.stop()\n",
    "\n",
    "    def result(self):\n",
    "        if self.r_time is None or self.memory is None:\n",
    "            raise ResourceMonitorError(\"No resources monitored yet.\")\n",
    "        return ResourceUsage(name=self.name, r_time=self.r_time, memory=self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = ResourceMonitor()\n",
    "with rm:\n",
    "    x = 10 ** 6\n",
    "print(rm.result())\n",
    "# Output:\n",
    "# Resource usage for None:\n",
    "#   Time [s]: 2.917e-06\n",
    "#   Memory [b]: 8.7738037109375e-05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval_bml Horizon Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def eval_bml_horizon_default(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    for batch_number, batch_df in test.groupby(np.arange(len(test)) // horizon):\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            preds = pd.Series(model.predict(batch_df.loc[:, batch_df.columns != target_column]))\n",
    "        diffs = batch_df[target_column].values - preds\n",
    "        df_eval.loc[batch_number + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=batch_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval BML New:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bml_horizon(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    # Reset index of train and test dataframes\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    # Initialize lists for predictions and differences\n",
    "    preds_list = []\n",
    "    diffs_list = []\n",
    "    # Fit the model on the training data\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    # Evaluate the model on empty arrays to get initial resource usage\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    # If include_remainder is False, remove remainder rows from test dataframe\n",
    "    if include_remainder is False:\n",
    "        remainder = len(test) % horizon\n",
    "        if remainder > 0:\n",
    "            test = test[:-remainder]\n",
    "    # Evaluate the model on batches of size horizon from the test dataframe\n",
    "    for batch_number, batch_df in test.groupby(np.arange(len(test)) // horizon):\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            preds = model.predict(batch_df.loc[:, batch_df.columns != target_column])\n",
    "        diffs = batch_df[target_column].values - preds\n",
    "        df_eval.loc[batch_number + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=batch_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        # Append predictions and differences to their respective lists\n",
    "        preds_list.append(preds)\n",
    "        diffs_list.append(diffs)\n",
    "\n",
    "    # Concatenate predictions and differences lists into series\n",
    "    series_preds = pd.Series(np.concatenate(preds_list))\n",
    "    series_diffs = pd.Series(np.concatenate(diffs_list))\n",
    "\n",
    "    # Create a dataframe with true values and add columns for predictions and differences\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create synthetic data for regression with 100 observations, 3 features and one target value\n",
    "X_train, y_train = make_regression(n_samples=80, n_features=3, n_targets=1)\n",
    "X_test, y_test = make_regression(n_samples=20, n_features=3, n_targets=1)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = mean_absolute_error\n",
    "df_eval, df_true = eval_bml_horizon(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create synthetic data for classification with 10 observations and one target value\n",
    "X_train, y_train = make_classification(n_samples=10, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=2, random_state=0)\n",
    "X_test, y_test = make_classification(n_samples=10, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=2, random_state=2)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=['Feature 1', 'Feature 2'])\n",
    "train['y'] = y_train\n",
    "test = pd.DataFrame(X_test, columns=['Feature 1', 'Feature 2'])\n",
    "test['y'] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = accuracy_score\n",
    "df_eval, df_true = eval_bml_horizon(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd example classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X_train, y_train = make_classification(n_samples=1000)\n",
    "X_test, y_test = make_classification(n_samples=1000)\n",
    "\n",
    "# Convert to DataFrames and add target column\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['target'] = y_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['target'] = y_test\n",
    "\n",
    "# Initialize Decision Tree Classifier and evaluate using eval_bml_horizon function\n",
    "dtc = DecisionTreeClassifier()\n",
    "eval_results_df, true_preds_diffs_df = eval_bml_horizon(dtc, train_df,test_df,'target', 10, True, accuracy_score)\n",
    "\n",
    "# Print first 5 rows of evaluation results DataFrame\n",
    "print(eval_results_df.head())\n",
    "\n",
    "# Print first 5 rows of true values/predictions/differences DataFrame\n",
    "print(true_preds_diffs_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BML Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Generator\n",
    "def gen_sliding_window(\n",
    "    df: pd.DataFrame, horizon: int, include_remainder: bool = True\n",
    ") -> Generator[pd.DataFrame, None, None]:\n",
    "    i = 0\n",
    "    while True:\n",
    "        subset = df[i * horizon : (i + 1) * horizon]\n",
    "        if len(subset) == 0:\n",
    "            break\n",
    "        elif len(subset) < horizon:\n",
    "            if include_remainder:\n",
    "                yield subset\n",
    "            break\n",
    "        i += 1\n",
    "        yield subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bml_landmark(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    # Landmark Evaluation\n",
    "    for i, new_df in enumerate(gen_sliding_window(test, horizon)):\n",
    "        train = pd.concat([train, new_df], ignore_index=True)\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            preds = pd.Series(model.predict(new_df.loc[:, new_df.columns != target_column]))\n",
    "            model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "\n",
    "        diffs = new_df[target_column].values - preds\n",
    "        df_eval.loc[i + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=new_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create synthetic data for regression with 100 observations, 3 features and one target value\n",
    "X_train, y_train = make_regression(n_samples=80, n_features=3, n_targets=1)\n",
    "X_test, y_test = make_regression(n_samples=20, n_features=3, n_targets=1)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = mean_absolute_error\n",
    "df_eval, df_true = eval_bml_landmark(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X_train, y_train = make_classification(n_samples=1000)\n",
    "X_test, y_test = make_classification(n_samples=1000)\n",
    "\n",
    "# Convert to DataFrames and add target column\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['target'] = y_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['target'] = y_test\n",
    "\n",
    "# Initialize Decision Tree Classifier and evaluate using eval_bml_landmark function \n",
    "dtc = DecisionTreeClassifier()\n",
    "eval_results_df, true_preds_diffs_df = eval_bml_landmark(dtc, train_df,test_df,'target', 100, True, accuracy_score)\n",
    "\n",
    "# Print first 5 rows of evaluation results DataFrame \n",
    "print(eval_results_df.head())\n",
    "\n",
    "# Print first 5 rows of true values/predictions/differences DataFrame \n",
    "print(true_preds_diffs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval BML Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_horizon_shifted_window(df, window_size, horizon):\n",
    "    i = 0\n",
    "    while True:\n",
    "        train_window = df[i * horizon : i * horizon + window_size]\n",
    "        test_window = df[i * horizon + window_size : (i + 1) * horizon + window_size]\n",
    "        if len(test_window) == 0:\n",
    "            break\n",
    "        elif len(test_window) < horizon:\n",
    "            yield train_window, test_window\n",
    "            break\n",
    "        i += 1\n",
    "        yield train_window, test_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bml_window(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    df_all = pd.concat([train, test], ignore_index=True)\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    for i, (w_train, w_test) in enumerate(gen_horizon_shifted_window(df_all, len(train), horizon)):\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            model.fit(w_train.loc[:, w_train.columns != target_column], w_train[target_column])\n",
    "            preds = pd.Series(model.predict(w_test.loc[:, w_test.columns != target_column]))\n",
    "\n",
    "        diffs = w_test[target_column].values - preds\n",
    "        df_eval.loc[i + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=w_test[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create synthetic data for regression with 100 observations, 3 features and one target value\n",
    "X_train, y_train = make_regression(n_samples=80, n_features=3, n_targets=1)\n",
    "X_test, y_test = make_regression(n_samples=20, n_features=3, n_targets=1)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = mean_absolute_error\n",
    "df_eval, df_true = eval_bml_window(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X_train, y_train = make_classification(n_samples=1000)\n",
    "X_test, y_test = make_classification(n_samples=1000)\n",
    "\n",
    "# Convert to DataFrames and add target column\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['target'] = y_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['target'] = y_test\n",
    "\n",
    "# Initialize Decision Tree Classifier and evaluate using eval_bml_landmark function \n",
    "dtc = DecisionTreeClassifier()\n",
    "eval_results_df, true_preds_diffs_df = eval_bml_window(dtc, train_df,test_df,'target', 100, True, accuracy_score)\n",
    "\n",
    "# Print first 5 rows of evaluation results DataFrame \n",
    "print(eval_results_df.head())\n",
    "\n",
    "# Print first 5 rows of true values/predictions/differences DataFrame \n",
    "print(true_preds_diffs_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval OML Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream as river_stream\n",
    "def eval_oml_horizon(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    "    oml_grace_period: int = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Check if metric is None or null and raise ValueError if it is\n",
    "    if metric is None:\n",
    "        raise ValueError(\"The 'metric' parameter must not be None or null.\")\n",
    "    if oml_grace_period is None:\n",
    "        oml_grace_period = horizon\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "\n",
    "    # Initial Training on Train Data\n",
    "    # For OML, this is performed on a limited subset only (oml_grace_period).\n",
    "    train_X = train.loc[:, train.columns != target_column]\n",
    "    train_y = train[target_column]\n",
    "    train_X = train_X.tail(oml_grace_period)\n",
    "    train_y = train_y.tail(oml_grace_period)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        for xi, yi in river_stream.iter_pandas(train_X, train_y):\n",
    "            # The following line returns y_pred, which is not used, therefore set to \"_\":\n",
    "            _ = model.predict_one(xi)\n",
    "            # metric = metric.update(yi, y_pred)\n",
    "            model = model.learn_one(xi, yi)\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "\n",
    "    # Test Data Evaluation\n",
    "    for i, new_df in enumerate(gen_sliding_window(test, horizon)):\n",
    "        preds = []\n",
    "        test_X = new_df.loc[:, new_df.columns != target_column]\n",
    "        test_y = new_df[target_column]\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            for xi, yi in river_stream.iter_pandas(test_X, test_y):\n",
    "                pred = model.predict_one(xi)\n",
    "                preds.append(pred)  # This is falsly measured with the ResourceMonitor\n",
    "                model = model.learn_one(xi, yi)\n",
    "        preds = pd.Series(preds)\n",
    "        diffs = new_df[target_column].values - preds\n",
    "        df_eval.loc[i + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=new_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model, preprocessing, datasets\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "from river import datasets\n",
    "import pandas as pd\n",
    "dataset = datasets.TrumpApproval()\n",
    "data_dict = {key: [] for key in list(dataset)[0][0].keys()}\n",
    "data_dict[\"Approval\"] = []\n",
    "for x in dataset:\n",
    "    for key, value in x[0].items():\n",
    "        data_dict[key].append(value)\n",
    "    data_dict[\"Approval\"].append(x[1])\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.rename(columns={'date': 'ordinal_date', 'Gallup': 'gallup', 'Ipsos': 'ipsos', 'Morning Consult': 'morning_consult', 'Rasmussen': 'rasmussen', 'YouGov': 'you_gov'}, inplace=True)\n",
    "train = df[0:499]\n",
    "test = df[500:-1]\n",
    "target_column = \"Approval\"\n",
    "horizon = 10\n",
    "oml_grace_period = 5\n",
    "include_remainder = True\n",
    "df_eval, df_preds = eval_oml_horizon(model=model,\n",
    "                                     train=train,\n",
    "                                     test=test,\n",
    "                                     target_column=target_column,\n",
    "                                     horizon=horizon,\n",
    "                                     include_remainder=include_remainder,\n",
    "                                     metric= mean_absolute_error,\n",
    "                                     oml_grace_period=oml_grace_period)\n",
    "df_eval.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 7*24\n",
    "k = 0.1\n",
    "n_total = int(k*100_000)\n",
    "p_1 = int(k*25_000)\n",
    "p_2 = int(k*50_000)\n",
    "position=(p_1, p_2)\n",
    "n_train = 1_000\n",
    "a = n_train + p_1 - 12\n",
    "b = a + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "dataset = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "     seed=123\n",
    ")\n",
    "data_dict = {key: [] for key in list(dataset.take(1))[0][0].keys()}\n",
    "data_dict[\"y\"] = []\n",
    "for x, y in dataset.take(n_total):\n",
    "    for key, value in x.items():\n",
    "        data_dict[key].append(value)\n",
    "    data_dict[\"y\"].append(y)\n",
    "df = pd.DataFrame(data_dict)\n",
    "# Add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column\n",
    "df.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\n",
    "train = df[:n_train]\n",
    "test = df[n_train:]\n",
    "target_column = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_lm = preprocessing.StandardScaler()\n",
    "oml_lm |= linear_model.LinearRegression()\n",
    "\n",
    "df_eval_oml_lm, df_true_oml_lm = eval_oml_horizon(model=oml_lm, train=train, test=test, target_column=\"y\", horizon=horizon, metric=mean_absolute_error, oml_grace_period=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_oml_lm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_oml_lm.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "dataset = synth.SEA(variant=0, seed=42)\n",
    "data_dict = {key: [] for key in list(dataset.take(1))[0][0].keys()}\n",
    "data_dict[\"y\"] = []\n",
    "for x, y in dataset.take(n_total):\n",
    "    for key, value in x.items():\n",
    "        data_dict[key].append(value)\n",
    "    data_dict[\"y\"].append(y)\n",
    "df = pd.DataFrame(data_dict)\n",
    "# Add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column\n",
    "df.columns = [f\"x{i}\" for i in range(1, 4)] + [\"y\"]\n",
    "df = df.apply(lambda x: x.astype(int) if x.dtype == bool else x)\n",
    "train = df[:n_train]\n",
    "test = df[n_train:]\n",
    "target_column = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_lm = preprocessing.StandardScaler()\n",
    "oml_lm |= linear_model.LogisticRegression()\n",
    "\n",
    "df_eval_oml_lm, df_true_oml_lm = eval_oml_horizon(model=oml_lm, train=train, test=test, target_column=\"y\", horizon=horizon, metric=accuracy_score, oml_grace_period=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_oml_lm.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_bml_oml_horizon_metrics(\n",
    "    df_eval: list[pd.DataFrame] = None,\n",
    "    df_labels: list = None,\n",
    "    log_x=False,\n",
    "    log_y=False,\n",
    "    cumulative=True,\n",
    "    grid=True,\n",
    "    fig_width=16,\n",
    "    fig_height=5,\n",
    "    metric=None,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    # Check if input dataframes are provided\n",
    "    if df_eval is not None:\n",
    "        df_list = copy.deepcopy(df_eval)\n",
    "        # Convert single dataframe input to a list if needed\n",
    "        if df_list.__class__ != list:\n",
    "            df_list = [df_list]\n",
    "        # Define metric names and titles\n",
    "        metric_name = metric.__class__.__name__\n",
    "        metrics = [\"Metric\", \"CompTime (s)\", \"Memory (MB)\"]\n",
    "        titles = [metric_name, \"Computation time (s)\", \"Memory (MB)\"]\n",
    "        # Create subplots with shared x-axis\n",
    "        fig, axes = plt.subplots(3, figsize=(fig_width, fig_height), constrained_layout=True, sharex=True)\n",
    "        # Loop over each dataframe in input list\n",
    "        for j, df in enumerate(df_list):\n",
    "            if cumulative:\n",
    "                # df.MAE = np.cumsum(df.MAE) / range(1, (1 + df.MAE.size))\n",
    "                df[\"Metric\"] = np.cumsum(df[\"Metric\"]) / range(1, (1 + df[\"Metric\"].size))\n",
    "                df[\"CompTime (s)\"] = np.cumsum(df[\"CompTime (s)\"])  # / range(1, (1 + df[\"CompTime (s)\"].size))\n",
    "                # df[\"Memory (MB)\"] = np.cumsum(df[\"Memory (MB)\"]) / range(1, (1 + df[\"Memory (MB)\"].size))\n",
    "            # Loop over each metric\n",
    "            for i in range(3):\n",
    "                # Assign label based on input or default value\n",
    "                if df_labels is None:\n",
    "                    label = f\"{j}\"\n",
    "                else:\n",
    "                    label = df_labels[j]\n",
    "                # Plot metric values against dataset names\n",
    "                axes[i].plot(df.index.values.tolist(), df[metrics[i]].values.tolist(), label=label, **kwargs)\n",
    "                # Set title and legend\n",
    "                axes[i].set_title(titles[i])\n",
    "                axes[i].legend(loc=\"upper right\")\n",
    "                axes[i].grid(grid)\n",
    "                # Set logarithmic scales if specified\n",
    "                if log_x:\n",
    "                    axes[i].set_xscale(\"log\")\n",
    "                if log_y:\n",
    "                    axes[i].set_yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels=[\"oml\"]\n",
    "plot_bml_oml_horizon_metrics(df_eval = [df_eval_oml_lm], df_labels=df_labels, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_oml_lm = df_true_oml_lm.apply(lambda x: x.astype(int) if x.dtype == bool else x)\n",
    "df_true_oml_lm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bml_oml_horizon_predictions(\n",
    "df_true: list[pd.DataFrame] = None,\n",
    "df_labels: list = None,\n",
    "target_column: str = \"Actual\",\n",
    "log_x=False,\n",
    "log_y=False,\n",
    "skip_first_n=0,\n",
    "grid=True,\n",
    "fig_width=16,\n",
    "fig_height=5,\n",
    "**kwargs,\n",
    ") -> None:\n",
    "    if df_true is not None:\n",
    "        df_plot = copy.deepcopy(df_true)\n",
    "    if df_plot.__class__ != list:\n",
    "        df_plot = [df_plot]\n",
    "    # plot actual vs predicted values\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    for j, df in enumerate(df_plot):\n",
    "        # Assign label based on input or default value\n",
    "        if df_labels is None:\n",
    "            label = f\"{j}\"\n",
    "        else:\n",
    "            label = df_labels[j]\n",
    "        # skip first n values\n",
    "        df[\"Prediction\"][range(skip_first_n)] = np.nan\n",
    "        plt.plot(df.index, df[\"Prediction\"], label=label, **kwargs)\n",
    "    # Plot the actual value only once:\n",
    "    plt.plot(df_plot[0].index, df_plot[0][target_column], label=\"Actual\", **kwargs)\n",
    "    plt.title(\"Actual vs Prediction\")\n",
    "    if log_x:\n",
    "        plt.xscale(\"log\")\n",
    "    if log_y:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.grid(grid)\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bml_oml_horizon_predictions(df_true = [df_true_oml_lm[0:150]], target_column=target_column,  df_labels=df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'abc': [1, 2, 3], 'B': [4, 5, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the first n rows in column \"abc\" to np.nan\n",
    "n = 1\n",
    "df.loc[:n-1,'abc'] = np.nan\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bml_oml_horizon_predictions(\n",
    "    df_true: list[pd.DataFrame] = None,\n",
    "    df_labels: list = None,\n",
    "    target_column: str = \"Actual\",\n",
    "    log_x=False,\n",
    "    log_y=False,\n",
    "    skip_first_n=0,\n",
    "    grid=True,\n",
    "    fig_width=16,\n",
    "    fig_height=5,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    if df_true is not None:\n",
    "        df_plot = copy.deepcopy(df_true)\n",
    "        if df_plot.__class__ != list:\n",
    "            df_plot = [df_plot]\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        for j, df in enumerate(df_plot):\n",
    "            if df_labels is None:\n",
    "                label = f\"{j}\"\n",
    "            else:\n",
    "                label = df_labels[j]\n",
    "            df.loc[: skip_first_n - 1, \"Prediction\"] = np.nan\n",
    "            plt.plot(df.index, df[\"Prediction\"], label=label, **kwargs)\n",
    "        plt.plot(df_plot[0].index, df_plot[0][target_column], label=\"Actual\", **kwargs)\n",
    "        plt.title(\"Actual vs Prediction\")\n",
    "        if log_x:\n",
    "            plt.xscale(\"log\")\n",
    "        if log_y:\n",
    "            plt.yscale(\"log\")\n",
    "        plt.grid(grid)\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import math\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotPython.spot import spot\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "from spotRiver.utils.selectors import select_leaf_prediction, select_leaf_model\n",
    "from spotRiver import data\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from river import datasets, time_series, utils, compose, linear_model, optim, preprocessing, evaluate, metrics, tree \n",
    "from river.datasets import synth\n",
    "import copy\n",
    "import warnings\n",
    "import numbers\n",
    "from sklearn.preprocessing import OneHotEncoder , MinMaxScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline , Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bike Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.bike_sharing import get_bike_sharing_data\n",
    "import river.stream as river_stream\n",
    "import copy\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "df, train, test = get_bike_sharing_data()\n",
    "target_column=\"count\"\n",
    "n_samples = df.shape[0]\n",
    "X = copy.deepcopy(train)\n",
    "y = X.pop(\"count\")\n",
    "data = river_stream.iter_pandas(X, y)\n",
    "dataset = list(data)\n",
    "#\n",
    "categorical_columns = [\n",
    "    \"weather\",\n",
    "    \"season\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "]\n",
    "categories = [\n",
    "    [\"clear\", \"misty\", \"rain\"],\n",
    "    [\"spring\", \"summer\", \"fall\", \"winter\"],\n",
    "    [\"False\", \"True\"],\n",
    "    [\"False\", \"True\"],\n",
    "]\n",
    "\n",
    "m = test.shape[0]\n",
    "a = int(m/2)-100\n",
    "b = int(m/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trump Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error as mae\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from river import linear_model\n",
    "# from river import preprocessing, datasets\n",
    "# from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "# from river import datasets\n",
    "# import pandas as pd\n",
    "# from spotRiver.utils.data_conversion import convert_to_df\n",
    "# dataset = datasets.TrumpApproval()\n",
    "# target_column = \"Approval\"\n",
    "# df = convert_to_df(dataset, target_column)\n",
    "# df.rename(columns={'date': 'ordinal_date', 'Gallup': 'gallup', 'Ipsos': 'ipsos', 'Morning Consult': 'morning_consult', 'Rasmussen': 'rasmussen', 'YouGov': 'you_gov'}, inplace=True)\n",
    "# train = df[0:499]\n",
    "# test = df[500:-1]\n",
    "## model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree.splitter import EBSTSplitter, QOSplitter, TEBSTSplitter, GaussianSplitter, HistogramSplitter\n",
    "from river.linear_model import LinearRegression, PARegressor, Perceptron\n",
    "from river.tree import HoeffdingTreeRegressor\n",
    "from river.preprocessing import StandardScaler\n",
    "from river.compose import Pipeline\n",
    "from numpy import power\n",
    "from spotPython.utils.convert import class_for_name\n",
    "import json\n",
    "import numpy as np\n",
    "from spotRiver.data.river_hyper_dict import HyperDict\n",
    "from spotRiver.utils.assignments import assign_values, iterate_dict_values, convert_keys\n",
    "fun_control = {}\n",
    "# default hyper_dict for all algorithms, hyperparameters and levels\n",
    "# river_hyper_dict = HyperDict().load()\n",
    "# Load loacal hyper_dict:\n",
    "with open(\"river_hyper_dict.json\", \"r\") as f:\n",
    "         river_hyper_dict = json.load(f)\n",
    "core_model = HoeffdingTreeRegressor\n",
    "fun_control.update({\"core_model\": core_model})\n",
    "algorithm = \"HoeffdingTreeRegressor\"\n",
    "algorithm_hyper_dict = river_hyper_dict[algorithm]\n",
    "hyperparameter = \"leaf_prediction\"\n",
    "levels = [\"mean\", \"model\", \"adaptive\"]\n",
    "algorithm_hyper_dict[hyperparameter].update({\"levels\": levels})\n",
    "lower = 0\n",
    "upper = len(algorithm_hyper_dict[hyperparameter]) - 1\n",
    "fun_control.update({\"algorithm_hyper_dict\": algorithm_hyper_dict})\n",
    "fun_control.update({\"core_model_name\": algorithm})\n",
    "prep_model = StandardScaler()\n",
    "fun_control.update({\"prep_model\": prep_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: generate lower and upper bounds for each hyperparameter\n",
    "# fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function that takes the dictionary d and a key and returns the length of the \"levels\" list of the key. If the key is not in the dictionary, throw a value error.\n",
    "def get_upper_bound(d, key):\n",
    "    if key in d:\n",
    "        return len(d[key][\"levels\"]) - 1\n",
    "    else:\n",
    "        raise ValueError(\"Key not in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function that takes the dictionaries d and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from d. If the key value in the dictionary is 0, then take the first value from the list, if it is 1, take the second and so on. If a key is not in d, throw a value error. For example, if d= {\"HoeffdingTreeRegressor\": { \"leaf_prediction\": {\"levels\": [\"mean\", \"model\", \"adaptive\"],                           \"type\": \"factor\",   \"default\": \"mean\",                            \"river_parameter_type\": \"str\"},        \"leaf_model\": {\"levels\": [\"LinearRegression\", \"PARegressor\", \"Perceptron\"],                        \"type\": \"factor\",                        \"default\": \"LinearRegression\", \"river_parameter_type\": \"instance\"},        \"splitter\": {\"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],     \"type\": \"factor\",\"default\": \"EBSTSplitter\",                        \"river_parameter_type\": \"instance()\"},        \"binary_split\": {\"levels\": [0, 1],                        \"type\": \"factor\",                        \"default\": 0,                        \"river_parameter_type\": \"bool\"},        \"stop_mem_management\": {\"levels\": [0, 1],                                \"type\": \"factor\",                                \"default\": 0,                                \"river_parameter_type\": \"bool\"}    }} and v is {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 0, 'leaf_model': 0, 'model_selector_decay': 0.95, 'splitter': 1, 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0} then the function should return {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': 'LinearRegression', 'model_selector_decay': 0.95, 'splitter': 'TEBSTSplitter', 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0}\n",
    "# def get_dict_with_levels(d, v):\n",
    "#     new_dict = {}\n",
    "#     for key, value in v.items():\n",
    "#         if key in d:\n",
    "#             new_dict[key] = d[key][\"levels\"][value]\n",
    "#         else:\n",
    "#             new_dict[key] = v[key]\n",
    "#     return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function that takes the dictionaries d and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from d. If the key value in the dictionary is 0, then take the first value from the list, if it is 1, take the second and so on. If a key is not in d, take the key from v. If the river_parameter_type value is instance, return the class of the value from the module river via getattr(\"river\", value). For example, if d= {\"HoeffdingTreeRegressor\": { \"leaf_prediction\": {\"levels\": [\"mean\", \"model\", \"adaptive\"],                           \"type\": \"factor\",   \"default\": \"mean\",                            \"river_parameter_type\": \"str\"},        \"leaf_model\": {\"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],                        \"type\": \"factor\",                        \"default\": \"LinearRegression\", \"river_parameter_type\": \"instance\"},        \"splitter\": {\"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],     \"type\": \"factor\",\"default\": \"EBSTSplitter\",                        \"river_parameter_type\": \"instance()\"},        \"binary_split\": {\"levels\": [0, 1],                        \"type\": \"factor\",                        \"default\": 0,                        \"river_parameter_type\": \"bool\"},        \"stop_mem_management\": {\"levels\": [0, 1],                                \"type\": \"factor\",                                \"default\": 0,                                \"river_parameter_type\": \"bool\"}    }} and v is {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 0, 'leaf_model': 0, 'model_selector_decay': 0.95, 'splitter': 1, 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0} then the function should return {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': linear_model.LinearRegression, 'model_selector_decay': 0.95, 'splitter': 'TEBSTSplitter', 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0}.   Note that the value of the key \"leaf_model\" is a class and not a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "\n",
    "# def class_for_name(module_name, class_name) -> object:\n",
    "#     \"\"\"Returns a class for a given module and class name.\n",
    "\n",
    "#     Parameters:\n",
    "#         module_name (str): The name of the module.\n",
    "#         class_name (str): The name of the class.\n",
    "\n",
    "#     Returns:\n",
    "#         object: The class.\n",
    "\n",
    "#     Example:\n",
    "# from spotPython.utils.convert import class_for_name\n",
    "#             from scipy.optimize import rosen\n",
    "#             bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
    "#             shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n",
    "#             result = shgo_class(rosen, bounds)\n",
    "#     \"\"\"\n",
    "#     m = importlib.import_module(module_name)\n",
    "#     print(m)\n",
    "#     c = getattr(m, class_name)\n",
    "#     print(c)\n",
    "#     return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotPython.utils.convert import class_for_name\n",
    "def get_dict_with_levels_and_types(d, v):\n",
    "    new_dict = {}\n",
    "    for key, value in v.items():\n",
    "        if key in d and d[key][\"type\"] == \"factor\":\n",
    "            if d[key][\"river_parameter_type\"] == \"instance\":\n",
    "                print(d[key][\"levels\"][value])\n",
    "                # remove the last part of [\"a.b.c\"] and unlist it. For example, [\"a.b.c\"] becomes \"a.b\"\n",
    "                # mdl = \".\".join(d[key][\"levels\"][value].split(\".\")[:-1])\n",
    "                # if d[key][\"class_name\"] exists, use it, otherwise skip the command\n",
    "                if \"class_name\" in d[key]:\n",
    "                    mdl = d[key][\"class_name\"]\n",
    "                print(\"mdl\", mdl)\n",
    "                # select the last part of the string \"a.b.c\"\n",
    "                ## c = d[key][\"levels\"][value].split(\".\")[-1]\n",
    "                c = d[key][\"levels\"][value]\n",
    "                print(\"c\", c)\n",
    "                new_dict[key] = class_for_name(mdl, c)\n",
    "            elif d[key][\"river_parameter_type\"] == \"instance()\":\n",
    "                # mdl = \".\".join(d[key][\"levels\"][value].split(\".\")[:-1])\n",
    "                mdl = d[key][\"class_name\"]\n",
    "                print(mdl)\n",
    "                # select the last part of the string \"a.b.c\"\n",
    "                # c = d[key][\"levels\"][value].split(\".\")[-1]\n",
    "                c = d[key][\"levels\"][value]\n",
    "                print(c)\n",
    "                k = class_for_name(mdl, c)\n",
    "                new_dict[key] = k()\n",
    "            else:\n",
    "                new_dict[key] = d[key][\"levels\"][value]\n",
    "        else:\n",
    "            new_dict[key] = v[key]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the values from the \"default\" keys  from the dictionary fun_control[\"algorithm_hyper_dict\"] as a list. if the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type  \n",
    "def get_default_values_from_dict(d):\n",
    "    new_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if value[\"type\"] == \"int\":\n",
    "            new_dict[key] = int(value[\"default\"])\n",
    "        elif value[\"type\"] == \"float\":\n",
    "            new_dict[key] = float(value[\"default\"])\n",
    "        else:\n",
    "            new_dict[key] = value[\"default\"]\n",
    "    return new_dict\n",
    "default_dict = get_default_values_from_dict(fun_control[\"algorithm_hyper_dict\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[fun_control[\"algorithm_hyper_dict\"][key][\"default\"] for key in fun_control[\"algorithm_hyper_dict\"].keys()]])\n",
    "# X.shape[1]\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fun_control.update({\"var_name\": list(fun_control[\"algorithm_hyper_dict\"].keys()),\n",
    "#            \"var_type\": list(fun_control[\"algorithm_hyper_dict\"][key][\"type\"] for key in fun_control[\"algorithm_hyper_dict\"].keys())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type = list(fun_control[\"algorithm_hyper_dict\"][key][\"type\"] for key in fun_control[\"algorithm_hyper_dict\"].keys())\n",
    "var_name = list(fun_control[\"algorithm_hyper_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "n_samples = 500 # Trump train and test\n",
    "horizon = 7*24\n",
    "oml_grace_period = 2\n",
    "fun = HyperRiver(seed=123, log_level=50).fun_oml_horizon\n",
    "fun_control.update({\"data\": None, # dataset,\n",
    "               \"train\": train,\n",
    "               \"test\": test,\n",
    "               \"target_column\": target_column,\n",
    "               \"horizon\": horizon,\n",
    "               \"oml_grace_period\": oml_grace_period,\n",
    "               \"n_samples\": n_samples,\n",
    "               \"weights\": np.array([1, 1/1000, 1/1000])*10_000.0,\n",
    "               \"step\": 100,\n",
    "               \"log_level\": 50,\n",
    "               \"weight_coeff\": 1.0,\n",
    "               \"metric\": metrics.MAE(),\n",
    "               \"var_name\": var_name,\n",
    "               \"var_type\": var_type,\n",
    "               \"prep_model\": prep_model,\n",
    "               \"core_model\": HoeffdingTreeRegressor,\n",
    "               \"model_name\": \"HoeffdingTreeRegressor\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that generates a list from a dictionary. It takes the values from the keys \"upper\" in the dictionary and returns a list of the values in the same order as the keys in the dictionary. For example if the dictionary is {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}} the function should return the list [1, 2].\n",
    "def get_lower_values_from_dict(d):\n",
    "    lower = []\n",
    "    for key, value in d.items():\n",
    "        lower.append(value[\"lower\"])\n",
    "    return lower\n",
    "lower = get_lower_values_from_dict(fun_control[\"algorithm_hyper_dict\"])\n",
    "print(lower)\n",
    "def get_upper_values_from_dict(d):\n",
    "    upper = []\n",
    "    for key, value in d.items():\n",
    "        upper.append(value[\"upper\"])\n",
    "    return upper\n",
    "upper = get_upper_values_from_dict(fun_control[\"algorithm_hyper_dict\"])\n",
    "print(upper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_values_from_dict(d):\n",
    "    new_list = []\n",
    "    for key, value in d.items():\n",
    "        new_list.append(value[\"transform\"])\n",
    "    return new_list\n",
    "t_val = get_transform_values_from_dict(fun_control[\"algorithm_hyper_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"]())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control.update({\"data\": None, # dataset,\n",
    "               \"train\": train,\n",
    "               \"test\": test,\n",
    "               \"target_column\": target_column,\n",
    "               \"horizon\": horizon,\n",
    "               \"oml_grace_period\": oml_grace_period,\n",
    "               \"n_samples\": n_samples,\n",
    "               \"weights\": np.array([1, 1/1000, 1/1000])*10_000.0,\n",
    "               \"step\": 100,\n",
    "               \"log_level\": 50,\n",
    "               \"weight_coeff\": 1.0,\n",
    "               \"metric\": metrics.MAE(),\n",
    "               \"var_name\": var_name,\n",
    "               \"var_type\": var_type,\n",
    "               \"prep_model\": prep_model,\n",
    "               \"core_model\": HoeffdingTreeRegressor,\n",
    "               \"model_name\": \"HoeffdingTreeRegressor\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let lower = [10, 2, 1e-08, 0.01, 0, 0, 0.9, 0, 2, 0, 100, 0.01, 0, 100.0, 100000, 0, 0, 0] and upper = [1000, 20, 1e-06, 0.1, 2, 2, 0.99, 2, 10, 1, 500, 0.1, 1, 1000.0, 1000000, 1, 1, 0]. Find entries that are equal. The ouput should be a list where 0 represents that the values are not equal and 1 represents that are equal.\n",
    "def find_equal_in_lists(lower, upper):\n",
    "    equal = []\n",
    "    for i in range(len(lower)):\n",
    "        if lower[i] == upper[i]:\n",
    "            equal.append(1)\n",
    "        else:\n",
    "            equal.append(0)\n",
    "    return equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 7*24\n",
    "oml_grace_period = 2\n",
    "fun = HyperRiver(seed=123, log_level=50).fun_oml_horizon\n",
    "from spotPython.spot import spot\n",
    "from math import inf\n",
    "spot_htr = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   fun_repeats = 1,\n",
    "                   max_time = 5,\n",
    "                   noise = False,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=var_type,\n",
    "                   var_name=var_name,\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=123,\n",
    "                   log_level = 50,\n",
    "                   show_models= False,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 20,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      \"log_level\": 50\n",
    "                                      })\n",
    "spot_htr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "horizon = 10\n",
    "oml_grace_period = 2\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, oml_grace_period, metric=metric)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a new X arrives as a numerical value from SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leaf_prediction_test_value = 0\n",
    "X = np.array([[fun_control[\"algorithm_hyper_dict\"][key][\"default\"] for key in fun_control[\"algorithm_hyper_dict\"].keys()]])\n",
    "# if entry in X is a \"string\" replace it with the numerical value zero (0)\n",
    "X = np.where(X == \"string\", 0, X)\n",
    "X.shape[1]\n",
    "print(\"X\", X)\n",
    "var_dict = assign_values(X, fun_control[\"var_name\"])\n",
    "for values in iterate_dict_values(var_dict):\n",
    "            values = convert_keys(values, fun_control[\"var_type\"])\n",
    "            print(fun_control[\"algorithm_hyper_dict\"])\n",
    "            print(\"values:\", values)\n",
    "            ## values = apply_selectors(values, fun_control[\"core_model\"].__name__, fun_control[\"algorithm_hyper_dict\"])\n",
    "            values = get_dict_with_levels_and_types(d=fun_control[\"algorithm_hyper_dict\"], v=values)\n",
    "            print(\"values:\", values)\n",
    "            ## model = river.compose.Pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values))\n",
    "            model = Pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"]())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dict_with_levels(d=fun_control[\"algorithm_hyper_dict\"], v=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = get_dict_with_levels_and_types(d=fun_control[\"algorithm_hyper_dict\"], v=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model\n",
    "from river import preprocessing, datasets\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from river import datasets\n",
    "import pandas as pd\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "dataset = datasets.TrumpApproval()\n",
    "target_column = \"Approval\"\n",
    "df = convert_to_df(dataset, target_column)\n",
    "df.rename(columns={'date': 'ordinal_date', 'Gallup': 'gallup', 'Ipsos': 'ipsos', 'Morning Consult': 'morning_consult', 'Rasmussen': 'rasmussen', 'YouGov': 'you_gov'}, inplace=True)\n",
    "train = df[0:499]\n",
    "test = df[500:-1]\n",
    "## model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "metric = mae\n",
    "horizon = 10\n",
    "oml_grace_period = 2\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, oml_grace_period, metric=metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotRiver.data.river_hyper_dict import HyperDict\n",
    "from spotPython.hyperparameters.categorical import one_hot_encode, sum_encoded_values, get_one_hot, add_missing_elements,find_closest_key\n",
    "from spotRiver.utils.assignments import assign_values, iterate_dict_values, convert_keys\n",
    "river_hyper_dict = HyperDict().load()\n",
    "fun_control = {}\n",
    "hyper_dict = {}\n",
    "algorithm = \"HoeffdingTreeRegressor\"\n",
    "hyperparameter = \"leaf_prediction\"\n",
    "# leaf_prediction_dict = get_one_hot(algorithm, hyperparameter,filename=\"./river_hyper_dict.json\")\n",
    "leaf_prediction_dict = get_one_hot(algorithm, hyperparameter, d=river_hyper_dict)\n",
    "print(\"leaf_prediction_dict:\", leaf_prediction_dict)\n",
    "hyper_dict[hyperparameter] = leaf_prediction_dict\n",
    "print(\"hyper_dict:\", hyper_dict)\n",
    "fun_control[\"hyper_dict\"] = hyper_dict\n",
    "print(\"fun_control:\", fun_control)\n",
    "lb = [\"mean\"]\n",
    "ub = [\"adaptive\"]\n",
    "add_missing_elements(lb, ub)\n",
    "print(\"lb:\", lb)\n",
    "print(\"ub:\", ub)\n",
    "a = sum_encoded_values(lb, fun_control[\"hyper_dict\"][\"leaf_prediction\"])\n",
    "b = sum_encoded_values(ub, fun_control[\"hyper_dict\"][\"leaf_prediction\"])\n",
    "leaf_prediction_lower = min(a,b)\n",
    "leaf_prediction_upper = max(a,b)\n",
    "print(\"leaf_prediction_lower:\", leaf_prediction_lower)\n",
    "print(\"leaf_prediction_upper:\", leaf_prediction_upper)\n",
    "#\n",
    "leaf_prediction_test_value = 4\n",
    "X = np.array([200,\n",
    "                  6,\n",
    "                  1e-6,\n",
    "                  0.075,\n",
    "                  leaf_prediction_test_value,\n",
    "                  -1,\n",
    "                  0.975,\n",
    "                  -1,\n",
    "                  10,\n",
    "                  1,\n",
    "                  750.0])\n",
    "fun_control.update({\"var_name\": [\"grace_period\",\n",
    "          \"max_depth\",\n",
    "          \"delta\",\n",
    "          \"tau\",\n",
    "          \"leaf_prediction\",\n",
    "          \"leaf_model\",\n",
    "          \"model_selector_decay\",\n",
    "          \"splitter\",\n",
    "          \"min_samples_split\",\n",
    "          \"binary_split\",\n",
    "          \"max_size\"],\n",
    "            \"var_type\": [\"int\",\n",
    "            \"int\",\n",
    "            \"num\",\n",
    "            \"num\",\n",
    "            \"factor\",\n",
    "            \"factor\",\n",
    "            \"num\",\n",
    "            \"factor\",\n",
    "            \"int\",\n",
    "            \"factor\",\n",
    "            \"num\"]})\n",
    "print(\"fun_control\", fun_control)\n",
    "X = np.array([X])\n",
    "leaf_prediction_test_value = 6\n",
    "X = np.array([[200, 10, 1e-07, 0.05, leaf_prediction_test_value, 0, 0.95, 1, 9, 0, 500.0]])\n",
    "X.shape[1]\n",
    "# print(\"X:\", X)\n",
    "# print(\"fun_control[var_name]:\",fun_control[\"var_name\"])\n",
    "var_dict = assign_values(X, fun_control[\"var_name\"])\n",
    "for values in iterate_dict_values(var_dict):\n",
    "            values = convert_keys(values, fun_control[\"var_type\"])\n",
    "            ## values = apply_selectors(values, fun_control[\"core_model\"].__name__, fun_control[\"hyper_dict\"])\n",
    "d = values\n",
    "print(d)\n",
    "hyper_dict = fun_control[\"hyper_dict\"]\n",
    "d[\"leaf_prediction\"] = find_closest_key(d[\"leaf_prediction\"], hyper_dict[\"leaf_prediction\"])\n",
    "print(d[\"leaf_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests Hyperriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "\n",
    "def test_compute_y():\n",
    "    # create a Hyperriver object\n",
    "    hr = HyperRiver()\n",
    "\n",
    "    # create a sample evaluation DataFrame\n",
    "    df_eval = pd.DataFrame({\n",
    "        \"Metric\": [0.1, 0.2, 0.3],\n",
    "        \"CompTime (s)\": [1.0, 2.0, 3.0],\n",
    "        \"Memory (MB)\": [10.0, 20.0, 30.0]\n",
    "    })\n",
    "\n",
    "    # set the weights\n",
    "    hr.fun_control[\"weights\"] = [1, 2, 3]\n",
    "\n",
    "    # compute the objective function value\n",
    "    y = hr.compute_y(df_eval)\n",
    "\n",
    "    # check that the result is correct\n",
    "    assert y == 0.1*1 + 0.2*2 + 0.3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_compute_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "import pandas as pd\n",
    "hr = HyperRiver()\n",
    "df_eval = pd.DataFrame( [[1, 2, 3], [3, 4, 5]], columns=['Metric', 'CompTime (s)', 'Memory (MB)'])\n",
    "hr.compute_y(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from river import linear_model\n",
    "model = linear_model.LinearRegression(intercept_lr=.1)\n",
    "train = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [2, 4, 6]})\n",
    "test = pd.DataFrame({\"x\": [4, 5], \"y\": [8, 10]})\n",
    "df_eval, df_true = eval_oml_horizon(model, train, test, \"y\", horizon=1, metric=mean_absolute_error)\n",
    "print(train)\n",
    "print(test)\n",
    "print(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream as river_stream\n",
    "import pandas as pd\n",
    "from spotRiver.evaluation.eval_bml import evaluate_model, ResourceMonitor, gen_sliding_window\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "from river import linear_model\n",
    "model = linear_model.LinearRegression(intercept_lr=.1)\n",
    "metric = mean_absolute_error\n",
    "horizon = 1\n",
    "train = pd.DataFrame({\"x\": [1, 2, 3], \"y\": [2, 4, 6]})\n",
    "test = pd.DataFrame({\"x\": [4, 5], \"y\": [8, 10]})\n",
    "series_preds = pd.Series(dtype=float)\n",
    "series_diffs = pd.Series(dtype=float)\n",
    "target_column = \"y\"\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "train_X = train.loc[:, train.columns != target_column]\n",
    "train_y = train[target_column]\n",
    "rm = ResourceMonitor()\n",
    "with rm:\n",
    "    for xi, yi in river_stream.iter_pandas(train_X, train_y):\n",
    "        # The following line returns y_pred, which is not used, therefore set to \"_\":\n",
    "        _ = model.predict_one(xi)\n",
    "        # metric = metric.update(yi, y_pred)\n",
    "        model = model.learn_one(xi, yi)\n",
    "        print(model.intercept)\n",
    "        print(model.weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old version 0.1.5\n",
    "\n",
    "# df_eval = pd.DataFrame.from_dict(\n",
    "#     [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "# )\n",
    "# df_eval\n",
    "# # Test Data Evaluation\n",
    "# for i, new_df in enumerate(gen_sliding_window(test, horizon)):\n",
    "#     preds = []\n",
    "#     test_X = new_df.loc[:, new_df.columns != target_column]\n",
    "#     test_y = new_df[target_column]\n",
    "#     rm = ResourceMonitor()\n",
    "#     with rm:\n",
    "#         for xi, yi in river_stream.iter_pandas(test_X, test_y):\n",
    "#             pred = model.predict_one(xi)\n",
    "#             preds.append(pred)  # This is falsely measured with the ResourceMonitor\n",
    "#             model = model.learn_one(xi, yi)\n",
    "#     preds = pd.Series(preds)\n",
    "#     diffs = new_df[target_column].values - preds\n",
    "#     df_eval.loc[i + 1] = pd.Series(\n",
    "#         evaluate_model(\n",
    "#             y_true=new_df[target_column],\n",
    "#             y_pred=preds,\n",
    "#             memory=rm.memory,\n",
    "#             r_time=rm.r_time,\n",
    "#             metric=metric,\n",
    "#         )\n",
    "#     )\n",
    "#     series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "#     series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "# df_true = pd.DataFrame(test[target_column])\n",
    "# df_true[\"Prediction\"] = series_preds\n",
    "# df_true[\"Difference\"] = series_diffs\n",
    "# df_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import stream as river_stream\n",
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "from river import evaluate, stream\n",
    "from river import metrics\n",
    "from spotRiver.evaluation.eval_bml import evaluate_model, ResourceMonitor, gen_sliding_window\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# TODO: Check, why the lin model with scaling does not work\n",
    "# model = linear_model.LinearRegression(intercept_lr=.1)\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LinearRegression(intercept_lr=.5)\n",
    ")\n",
    "\n",
    "horizon = 10\n",
    "\n",
    "train = pd.DataFrame({\"x\": np.arange(1, 11), \"y\": np.arange(2, 22, 2)})\n",
    "print(f\"Size of train: {train.shape}\")\n",
    "test = pd.DataFrame({\"x\": np.arange(11, 111), \"y\": np.arange(22, 222, 2)})\n",
    "print(f\"Size of test: {test.shape}\")\n",
    "target_column = \"y\"\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "train_X = train.loc[:, train.columns != target_column]\n",
    "train_y = train[target_column]\n",
    "\n",
    "# Fit the model on the train data\n",
    "# No predictions are made here, only the model is fitted\n",
    "# Memory and runtime are measured for the model fitting\n",
    "rm = ResourceMonitor()\n",
    "with rm:\n",
    "    for xi, yi in river_stream.iter_pandas(train_X, train_y):\n",
    "        # Before v0.19 we had to call predict_one before learn_one\n",
    "        # in order for the whole pipeline to be updated.\n",
    "        # Since v0.19, calling learn_one in a pipeline will update each part \n",
    "        # of the pipeline in turn. \n",
    "        # Before v0.19, predict_one has to be called for updating the unsupervised parts \n",
    "        # of the pipeline.\n",
    "        # The following line, which returns y_pred, which is not used after v0.19:\n",
    "        # _ = model.predict_one(xi)         \n",
    "        model = model.learn_one(xi, yi)\n",
    "\n",
    "# Create empty lists to collect data\n",
    "eval_data = []\n",
    "series_preds = []\n",
    "series_diffs = []\n",
    "\n",
    "# Add the evaluation of the model (memory and time, not predictions) on the train data to the eval_data list\n",
    "# A metric must not be passed to the evaluate_model function, because no predictions are made here\n",
    "# If a metric is passed, it will be ignored, because no predictions are passed to the evaluation function\n",
    "# So, metric=None and metric=mean_absolute_error will both work\n",
    "metric = mean_absolute_error\n",
    "eval_data.append(evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric))\n",
    "print(f\"eval_data: {eval_data}\")\n",
    "\n",
    "# Test Data Evaluation\n",
    "# A sliding window of length horizon is used to evaluate the model on the test data\n",
    "for i, new_df in enumerate(gen_sliding_window(test, horizon)):\n",
    "    print(\"i:\", i)\n",
    "    print(\"new_df:\", new_df)\n",
    "    preds = []\n",
    "    test_X = new_df.loc[:, new_df.columns != target_column]\n",
    "    test_y = new_df[target_column]\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        for xi, yi in river_stream.iter_pandas(test_X, test_y):\n",
    "            pred = model.predict_one(xi)\n",
    "            preds.append(pred)\n",
    "            model = model.learn_one(xi, yi)\n",
    "    preds = pd.Series(preds)\n",
    "    diffs = new_df[target_column].values - preds\n",
    "\n",
    "    # Collect data in lists\n",
    "    eval_data.append(evaluate_model(\n",
    "        y_true=new_df[target_column],\n",
    "        y_pred=preds,\n",
    "        memory=rm.memory,\n",
    "        r_time=rm.r_time,\n",
    "        metric=metric\n",
    "    ))\n",
    "    series_preds.extend(preds)\n",
    "    series_diffs.extend(diffs)\n",
    "\n",
    "# Create DataFrames from the collected data\n",
    "df_eval = pd.DataFrame(eval_data)\n",
    "df_true = pd.DataFrame(test[target_column])\n",
    "df_true[\"Prediction\"] = series_preds\n",
    "df_true[\"Difference\"] = series_diffs\n",
    "print(df_true)\n",
    "print(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a deep copy of the test and assign it to X\n",
    "X = copy.deepcopy(test)\n",
    "y = X.pop('y')\n",
    "dataset = stream.iter_pandas(X, y)\n",
    "# for xi, yi in dataset:\n",
    "#     print(xi, yi)\n",
    "metric = metrics.MAE()\n",
    "evaluate.progressive_val_score(dataset, model, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a deep copy of the test and assign it to X\n",
    "X = copy.deepcopy(test)\n",
    "y = X.pop('y')\n",
    "dataset = stream.iter_pandas(X, y)\n",
    "x, y = next(iter(dataset))\n",
    "report = model.debug_one(x)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(dataset))\n",
    "report = model.debug_one(x)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "metric = mean_absolute_error\n",
    "model = (\n",
    "        preprocessing.StandardScaler() |\n",
    "        linear_model.LinearRegression(intercept_lr=.5)\n",
    "    )\n",
    "horizon = 5\n",
    "train = pd.DataFrame({\"x\": np.arange(1, 11), \"y\": np.arange(2, 22, 2)})\n",
    "print(f\"Size of train: {train.shape}\")\n",
    "test = pd.DataFrame({\"x\": np.arange(11, 111), \"y\": np.arange(22, 222, 2)})\n",
    "print(f\"Size of test: {test.shape}\")\n",
    "target_column = \"y\"\n",
    "res, preds = eval_oml_horizon(\n",
    "    model = model,\n",
    "    train = train,\n",
    "    test = test,\n",
    "    target_column = target_column,\n",
    "    horizon = horizon,\n",
    "    include_remainder = True,\n",
    "    metric = metric,\n",
    "    oml_grace_period = horizon,\n",
    ")\n",
    "print(res.shape[0] == 1 + test.shape[0] // horizon)\n",
    "print(preds.shape == (test.shape[0],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: eval_oml_horizon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from river import preprocessing\n",
    "from river.forest import AMFClassifier\n",
    "from river.datasets import Bananas, CreditCard, Phishing\n",
    "from math import inf\n",
    "\n",
    "from spotRiver.data.river_hyper_dict import RiverHyperDict\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "from spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics\n",
    "from spotPython.plot.validation import plot_roc_from_dataframes\n",
    "from spotPython.plot.validation import plot_confusion_matrix\n",
    "from spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics\n",
    "\n",
    "\n",
    "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotPython.utils.init import fun_control_init\n",
    "from spotPython.utils.file import get_spot_tensorboard_path\n",
    "from spotPython.utils.file import get_experiment_name\n",
    "from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
    "from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotPython.spot import spot\n",
    "from spotPython.hyperparameters.values import (\n",
    "    get_var_type,\n",
    "    get_var_name,\n",
    "    get_bound_values\n",
    "    )\n",
    "from spotPython.utils.eda import gen_design_table\n",
    "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
    "from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotPython.utils.tensorboard import start_tensorboard, stop_tensorboard\n",
    "\n",
    "MAX_TIME = 1\n",
    "INIT_SIZE = 5\n",
    "PREFIX=\"0000-river\"\n",
    "horizon = 30\n",
    "n_samples = 1250\n",
    "n_train = 500\n",
    "oml_grace_period = n_train\n",
    "# data_set = \"Bananas\"\n",
    "# data_set = \"CreditCard\"\n",
    "data_set = \"Phishing\"\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    TENSORBOARD_CLEAN=True)\n",
    "#fun_control\n",
    "if data_set == \"Bananas\":\n",
    "    dataset = Bananas()\n",
    "elif data_set == \"CreditCard\":\n",
    "    dataset = CreditCard()\n",
    "elif data_set == \"Phishing\":\n",
    "    dataset = Phishing()\n",
    "else:\n",
    "    raise ValueError(\"data_set must be 'Bananas' or 'ConceptDriftStream'\")\n",
    "target_column = \"y\"\n",
    "weights = np.array([- 1, 1/1000, 1/1000])*10_000.0\n",
    "weight_coeff = 1.0\n",
    "df = convert_to_df(dataset, target_column=target_column, n_total=n_samples)\n",
    "df.columns = [f\"x{i}\" for i in range(1, dataset.n_features+1)] + [\"y\"]\n",
    "df[\"y\"] = df[\"y\"].astype(int)\n",
    "if prepmodel == \"StandardScaler\":\n",
    "    prep_model = preprocessing.StandardScaler()\n",
    "elif prepmodel == \"MinMaxScaler\":\n",
    "    prep_model = preprocessing.MinMaxScaler()\n",
    "else:\n",
    "    prep_model = None\n",
    "fun_control.update({\"train\":  df[:n_train],\n",
    "                    \"oml_grace_period\": oml_grace_period,\n",
    "                    \"test\":  df[n_train:],\n",
    "                    \"n_samples\": n_samples,\n",
    "                    \"target_column\": target_column,\n",
    "                    \"prep_model\": prep_model,\n",
    "                    \"horizon\": horizon,\n",
    "                    \"oml_grace_period\": oml_grace_period,\n",
    "                    \"weights\": weights,\n",
    "                    \"weight_coeff\": weight_coeff,\n",
    "                    \"metric_sklearn\": accuracy_score\n",
    "                    })\n",
    "add_core_model_to_fun_control(core_model=AMFClassifier,\n",
    "                              fun_control=fun_control,\n",
    "                              hyper_dict=RiverHyperDict,\n",
    "                              filename=None)\n",
    "modify_hyper_parameter_bounds(fun_control, \"n_estimators\", bounds=[2,20])\n",
    "modify_hyper_parameter_bounds(fun_control, \"step\", bounds=[0.5,2])\n",
    "\n",
    "\n",
    "X_start = get_default_hyperparameters_as_array(fun_control)\n",
    "fun = HyperRiver(log_level=50).fun_oml_horizon\n",
    "var_type = get_var_type(fun_control)\n",
    "var_name = get_var_name(fun_control)\n",
    "lower = get_bound_values(fun_control, \"lower\")\n",
    "upper = get_bound_values(fun_control, \"upper\")\n",
    "\n",
    "p_open = start_tensorboard()\n",
    "spot_tuner = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   max_time = MAX_TIME,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type = var_type,\n",
    "                   var_name = var_name,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": INIT_SIZE},\n",
    "                   surrogate_control={\"noise\": False,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_fun_evals\": 10_000},\n",
    "                                      log_level=50)\n",
    "spot_tuner.run(X_start=X_start)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tensorboard(p_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_tuner.plot_progress(log_y=False, filename=None)\n",
    "print(gen_design_table(fun_control=fun_control, spot=spot_tuner))\n",
    "spot_tuner.plot_importance(threshold=0.0025, filename=None)\n",
    "X = spot_tuner.to_all_dim(spot_tuner.min_X.reshape(1,-1))\n",
    "model_spot = get_one_core_model_from_X(X, fun_control)\n",
    "df_eval_spot, df_true_spot = eval_oml_horizon(\n",
    "                    model=model_spot,\n",
    "                    train=fun_control[\"train\"],\n",
    "                    test=fun_control[\"test\"],\n",
    "                    target_column=fun_control[\"target_column\"],\n",
    "                    horizon=fun_control[\"horizon\"],\n",
    "                    oml_grace_period=fun_control[\"oml_grace_period\"],\n",
    "                    metric=fun_control[\"metric_sklearn\"],\n",
    "                )\n",
    "X_start = get_default_hyperparameters_as_array(fun_control)\n",
    "model_default = get_one_core_model_from_X(X_start, fun_control)\n",
    "df_eval_default, df_true_default = eval_oml_horizon(\n",
    "                    model=model_default,\n",
    "                    train=fun_control[\"train\"],\n",
    "                    test=fun_control[\"test\"],\n",
    "                    target_column=fun_control[\"target_column\"],\n",
    "                    horizon=fun_control[\"horizon\"],\n",
    "                    oml_grace_period=fun_control[\"oml_grace_period\"],\n",
    "                    metric=fun_control[\"metric_sklearn\"],\n",
    "                )\n",
    "\n",
    "df_labels=[\"default\", \"spot\"]\n",
    "plot_bml_oml_horizon_metrics(df_eval = [df_eval_default, df_eval_spot], log_y=False, df_labels=df_labels, metric=fun_control[\"metric_sklearn\"], filename=None)\n",
    "\n",
    "spot_tuner.plot_important_hyperparameter_contour(filename=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_default = get_one_core_model_from_X(X_start, fun_control)\n",
    "df_eval_default, df_true_default = eval_oml_horizon(\n",
    "                    model=model_default,\n",
    "                    train=fun_control[\"train\"],\n",
    "                    test=fun_control[\"test\"],\n",
    "                    target_column=fun_control[\"target_column\"],\n",
    "                    horizon=fun_control[\"horizon\"],\n",
    "                    oml_grace_period=fun_control[\"oml_grace_period\"],\n",
    "                    metric=fun_control[\"metric_sklearn\"],\n",
    "                )\n",
    "df_eval_default.shape == ((n_samples - n_train) // horizon + 1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from river import preprocessing\n",
    "from river.forest import AMFClassifier\n",
    "from river.datasets import Bananas, synth\n",
    "from math import inf\n",
    "\n",
    "from spotRiver.data.river_hyper_dict import RiverHyperDict\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "\n",
    "from spotPython.hyperparameters.values import add_core_model_to_fun_control\n",
    "from spotPython.utils.init import fun_control_init\n",
    "from spotPython.utils.file import get_spot_tensorboard_path\n",
    "from spotPython.utils.file import get_experiment_name\n",
    "from spotPython.hyperparameters.values import modify_hyper_parameter_bounds\n",
    "from spotPython.hyperparameters.values import get_one_core_model_from_X\n",
    "from spotPython.hyperparameters.values import get_default_hyperparameters_as_array\n",
    "from spotPython.spot import spot\n",
    "from spotPython.hyperparameters.values import (\n",
    "    get_var_type,\n",
    "    get_var_name,\n",
    "    get_bound_values\n",
    "    )\n",
    "\n",
    "MAX_TIME = 1\n",
    "INIT_SIZE = 5\n",
    "PREFIX=\"0000-river\"\n",
    "horizon = 30\n",
    "n_samples = 5300\n",
    "n_train = 5000\n",
    "oml_grace_period = n_train\n",
    "data_set = \"Bananas\"\n",
    "#data_set = \"ConceptDriftStream\"\n",
    "\n",
    "\n",
    "experiment_name = get_experiment_name(prefix=PREFIX)\n",
    "fun_control = fun_control_init(\n",
    "    spot_tensorboard_path=get_spot_tensorboard_path(experiment_name),\n",
    "    TENSORBOARD_CLEAN=True)\n",
    "#fun_control\n",
    "dataset_b = Bananas()\n",
    "dataset_s = synth.ConceptDriftStream(\n",
    "    seed=42,\n",
    "    position=(n_samples // 2),\n",
    "    width=40).take(n_samples)\n",
    "# convert the dataset_s which is of type islice to a pandas DataFrame. The \n",
    "dataset_s = pd.DataFrame(dataset_s)\n",
    "dataset_s.columns = [f\"x{i}\" for i in range(1, dataset_s.shape[1])] + [\"y\"]\n",
    "dataset_s[\"y\"] = dataset_s[\"y\"].astype(int)\n",
    "dataset_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.tuner.run import run_spot_river_experiment\n",
    "run_spot_river_experiment(MAX_TIME=1,\n",
    "    INIT_SIZE=5,\n",
    "    PREFIX=\"0000-river\",\n",
    "    horizon=1,\n",
    "    n_samples=None,\n",
    "    n_train=None,\n",
    "    oml_grace_period=None,\n",
    "    data_set=\"Phishing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotPython.fun.objectivefunctions import analytical\n",
    "from spotPython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "\n",
    "fun = analytical().fun_sphere\n",
    "lower = np.array([-1])\n",
    "upper = np.array([1])\n",
    "design_control={\"init_size\": ni}\n",
    "\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            lower = lower,\n",
    "            upper= upper,\n",
    "            fun_evals = n,\n",
    "            show_progress=True,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve the following output from the vars command so that the output is nicely formatted and shown in a table: \n",
    "vars(spot_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def vars_to_dataframe(obj):\n",
    "    return pd.DataFrame.from_records([vars(obj)])\n",
    "\n",
    "df = vars_to_dataframe(spot_1)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars_to_dataframe(obj):\n",
    "    data = vars(obj)\n",
    "    for key, value in data.items():\n",
    "        if isinstance(value, dict):\n",
    "            data[key] = vars_to_dataframe(value)\n",
    "    return pd.DataFrame.from_records([data])\n",
    "\n",
    "df = vars_to_dataframe(spot_1)\n",
    "df_transposed = df.T\n",
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars_to_dataframe(obj):\n",
    "    if hasattr(obj, '__dict__'):\n",
    "        data = vars(obj)\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, dict):\n",
    "                data[key] = vars_to_dataframe(value)\n",
    "        return pd.DataFrame.from_records([data])\n",
    "    else:\n",
    "        return pd.DataFrame({})\n",
    "\n",
    "df = vars_to_dataframe(spot_1)\n",
    "df_transposed = df.T\n",
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars_to_dataframe(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        data = {k: vars_to_dataframe(v) if isinstance(v, dict) else v for k, v in obj.items()}\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        data = vars_to_dataframe(vars(obj))\n",
    "    else:\n",
    "        return obj\n",
    "    return pd.Series(data)\n",
    "\n",
    "df = vars_to_dataframe(spot_1).to_frame().T\n",
    "df_transposed = df.T\n",
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vars_to_dataframe(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        data = {k: vars_to_dataframe(v) if isinstance(v, dict) else str(v) for k, v in obj.items()}\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        data = vars_to_dataframe(vars(obj))\n",
    "    else:\n",
    "        return obj\n",
    "    return pd.Series(data)\n",
    "\n",
    "df = vars_to_dataframe(spot_1).to_frame().T\n",
    "df_transposed = df.T\n",
    "df_transposed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def dict_to_dataframe(d):\n",
    "    for key, value in d.items():\n",
    "        if isinstance(value, dict):\n",
    "            d[key] = dict_to_dataframe(value)\n",
    "    return pd.DataFrame([d])\n",
    "\n",
    "# Example usage:\n",
    "d = {'a': 1, 'b': {'c': 2, 'd': 3}}\n",
    "df = dict_to_dataframe(d)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import inf\n",
    "from spotPython.fun.objectivefunctions import analytical\n",
    "from spotPython.spot import spot\n",
    "# number of initial points:\n",
    "ni = 7\n",
    "# number of points\n",
    "n = 10\n",
    "\n",
    "fun = analytical().fun_sphere\n",
    "lower = np.array([-1])\n",
    "upper = np.array([1])\n",
    "design_control={\"init_size\": ni}\n",
    "\n",
    "spot_1 = spot.Spot(fun=fun,\n",
    "            lower = lower,\n",
    "            upper= upper,\n",
    "            fun_evals = n,\n",
    "            show_progress=True,\n",
    "            design_control=design_control,)\n",
    "spot_1.run()\n",
    "spot_1.get_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User specified data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "import importlib.resources as pkg_resources\n",
    "import spotRiver.data as data\n",
    "inp_file = pkg_resources.files(data)\n",
    "csv_path = str(inp_file.resolve())\n",
    "dataset = GenericData(filename=\"UnivariateData.csv\",\n",
    "                    directory=csv_path,\n",
    "                    target=\"Consumption\",\n",
    "                    n_features=1,\n",
    "                    n_samples=51_706,\n",
    "                    converters={\"Consumption\": float},\n",
    "                    parse_dates={\"Time\": \"%Y-%m-%d %H:%M:%S%z\"})\n",
    "for x, y in dataset:\n",
    "    print(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "import importlib.resources as pkg_resources\n",
    "import spotRiver.data as data\n",
    "csv_path = \"/Users/bartz/workspace/spotRiver/gui/userData\"\n",
    "dataset = GenericData(filename=\"user_data.csv\",\n",
    "                    directory=csv_path,\n",
    "                    target=\"Consumption\",\n",
    "                    n_features=1,\n",
    "                    n_samples=51_706,\n",
    "                    converters={\"Consumption\": float},\n",
    "                    parse_dates={\"Time\": \"%Y-%m-%d %H:%M:%S%z\"})\n",
    "for x, y in dataset:\n",
    "    print(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "csv_path = \"/Users/bartz/workspace/spotRiver/gui/userData\"\n",
    "dataset = GenericData(filename=\"PhishingData.csv\",\n",
    "                    directory=csv_path,\n",
    "                    target=\"is_phishing\",\n",
    "                    n_samples=1_250,\n",
    "                    n_features=9,\n",
    "                    converters={\n",
    "                        \"empty_server_form_handler\": float,\n",
    "                        \"popup_window\": float,\n",
    "                        \"https\": float,\n",
    "                        \"request_from_other_domain\": float,\n",
    "                        \"anchor_from_other_domain\": float,\n",
    "                        \"is_popular\": float,\n",
    "                        \"long_url\": float,\n",
    "                        \"age_of_domain\": int,\n",
    "                        \"ip_in_url\": int,\n",
    "                        \"is_phishing\": lambda x: x == \"1\",\n",
    "                    },\n",
    "                    parse_dates=None)\n",
    "for x, y in dataset:\n",
    "    print(x, y)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.generic import GenericData\n",
    "import importlib.resources as pkg_resources\n",
    "import spotRiver.data as data\n",
    "csv_path = \"/Users/bartz/workspace/spotRiver/gui/userData\"\n",
    "dataset = GenericData(filename=\"PhishingData.csv\",\n",
    "                    directory=csv_path,\n",
    "                    target=None,\n",
    "                    n_samples=1_250,\n",
    "                    n_features=9,\n",
    "                    converters=None,\n",
    "                    parse_dates=None)\n",
    "dataset.converters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x, y in dataset:\n",
    "    print(x, y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_set = Bananas\n",
      "n_samples = 5300\n",
      "(5300, 3)\n",
      "          1         2      y\n",
      "0  1.617466 -0.919233  False\n",
      "1 -1.394669  1.094125  False\n",
      "2 -2.321238  0.086109  False\n",
      "3 -0.215209 -0.216503   True\n",
      "4 -0.989363  0.479590  False\n",
      "data_set = CreditCard\n",
      "n_samples = 284807\n",
      "(284807, 31)\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  y  \n",
      "0 -0.189115  0.133558 -0.021053  149.62  0  \n",
      "1  0.125895 -0.008983  0.014724    2.69  0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66  0  \n",
      "3 -0.221929  0.062723  0.061458  123.50  0  \n",
      "4  0.502292  0.219422  0.215153   69.99  0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "data_set = Elec2\n",
      "n_samples = 45312\n",
      "(45312, 9)\n",
      "   date  day    period  nswprice  nswdemand  vicprice  vicdemand  transfer  \\\n",
      "0   0.0    2  0.000000  0.056443   0.439155  0.003467   0.422915  0.414912   \n",
      "1   0.0    2  0.021277  0.051699   0.415055  0.003467   0.422915  0.414912   \n",
      "2   0.0    2  0.042553  0.051489   0.385004  0.003467   0.422915  0.414912   \n",
      "3   0.0    2  0.063830  0.045485   0.314639  0.003467   0.422915  0.414912   \n",
      "4   0.0    2  0.085106  0.042482   0.251116  0.003467   0.422915  0.414912   \n",
      "\n",
      "       y  \n",
      "0   True  \n",
      "1   True  \n",
      "2   True  \n",
      "3   True  \n",
      "4  False  \n",
      "data_set = Higgs\n",
      "n_samples = 11000000\n",
      "(11000000, 29)\n",
      "   lepton pT  lepton eta  lepton phi  missing energy magnitude  \\\n",
      "0   0.869293   -0.635082    0.225690                  0.327470   \n",
      "1   0.907542    0.329147    0.359412                  1.497970   \n",
      "2   0.798835    1.470639   -1.635975                  0.453773   \n",
      "3   1.344385   -0.876626    0.935913                  1.992050   \n",
      "4   1.105009    0.321356    1.522401                  0.882808   \n",
      "\n",
      "   missing energy phi  jet 1 pt  jet 1 eta  jet 1 phi  jet 1 b-tag  jet 2 pt  \\\n",
      "0           -0.689993  0.754202  -0.248573  -1.092064     0.000000  1.374992   \n",
      "1           -0.313010  1.095531  -0.557525  -1.588230     2.173076  0.812581   \n",
      "2            0.425629  1.104875   1.282322   1.381664     0.000000  0.851737   \n",
      "3            0.882454  1.786066  -1.646778  -0.942383     0.000000  2.423265   \n",
      "4           -1.205349  0.681466  -1.070464  -0.921871     0.000000  0.800872   \n",
      "\n",
      "   ...  jet 4 phi  jet 4 b-tag      m_jj     m_jjj      m_lv     m_jlv  \\\n",
      "0  ...  -0.045767     3.101961  1.353760  0.979563  0.978076  0.920005   \n",
      "1  ...  -0.000819     0.000000  0.302220  0.833048  0.985700  0.978098   \n",
      "2  ...   0.900461     0.000000  0.909753  1.108330  0.985692  0.951331   \n",
      "3  ...  -1.360356     0.000000  0.946652  1.028704  0.998656  0.728281   \n",
      "4  ...   0.113041     0.000000  0.755856  1.361057  0.986610  0.838085   \n",
      "\n",
      "       m_bb     m_wbb    m_wwbb      y  \n",
      "0  0.721657  0.988751  0.876678   True  \n",
      "1  0.779732  0.992356  0.798343   True  \n",
      "2  0.803252  0.865924  0.780118   True  \n",
      "3  0.869200  1.026736  0.957904  False  \n",
      "4  1.133295  0.872245  0.808487   True  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "data_set = HTTP\n",
      "n_samples = 567498\n",
      "(567498, 4)\n",
      "   duration  src_bytes  dst_bytes  y\n",
      "0 -2.302585   5.371103  10.716107  0\n",
      "1 -2.302585   5.088213   8.418058  0\n",
      "2 -2.302585   5.464255   7.113224  0\n",
      "3 -2.302585   5.451468   7.616825  0\n",
      "4 -2.302585   5.476882   6.186414  0\n",
      "data_set = Phishing\n",
      "n_samples = 1250\n",
      "(1250, 10)\n",
      "   empty_server_form_handler  popup_window  https  request_from_other_domain  \\\n",
      "0                        0.0           0.0    0.0                        0.0   \n",
      "1                        1.0           0.0    0.5                        0.5   \n",
      "2                        0.0           0.0    1.0                        0.0   \n",
      "3                        0.0           0.0    1.0                        0.0   \n",
      "4                        1.0           0.0    0.5                        1.0   \n",
      "\n",
      "   anchor_from_other_domain  is_popular  long_url  age_of_domain  ip_in_url  \\\n",
      "0                       0.0         0.5       1.0              1          1   \n",
      "1                       0.0         0.5       0.0              1          0   \n",
      "2                       0.5         0.5       0.0              1          0   \n",
      "3                       0.0         1.0       0.5              0          0   \n",
      "4                       0.0         0.5       0.5              1          0   \n",
      "\n",
      "       y  \n",
      "0   True  \n",
      "1   True  \n",
      "2   True  \n",
      "3   True  \n",
      "4  False  \n"
     ]
    }
   ],
   "source": [
    "from spotRiver.data.selector import data_selector\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "data_set_values = [\n",
    "    \"Bananas\",\n",
    "    \"CreditCard\",\n",
    "    \"Elec2\",\n",
    "    \"Higgs\",\n",
    "    \"HTTP\",\n",
    "    # \"MaliciousURL\",\n",
    "    \"Phishing\",\n",
    "    # \"SMSSpam\",\n",
    "    # \"SMTP\",\n",
    "    # \"TREC07\"\n",
    "]\n",
    "\n",
    "# check if data_set provided by spotRiver as a data set from the river package\n",
    "for i in range(len(data_set_values)):\n",
    "    data_set = data_set_values[i]\n",
    "    print(f\"data_set = {data_set}\")\n",
    "    dataset, n_samples = data_selector(\n",
    "        data_set=data_set\n",
    "    )\n",
    "    print(f\"n_samples = {n_samples}\")\n",
    "    df = convert_to_df(dataset, target_column = \"y\", n_total=None)\n",
    "    print(df.shape)\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data if CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.selector import data_selector\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "data_set=\"phishingRiver.csv\"\n",
    "target_column = \"is_phishing\"\n",
    "n_total = None\n",
    "# check if data_set is provided as .csv\n",
    "if data_set.endswith(\".csv\"):\n",
    "    directory = \"./userData/\"\n",
    "    filename = data_set\n",
    "    target = target_column\n",
    "    # TODO: This is correct for the Phishing data set,\n",
    "    # but needs to be adapted for other data sets:\n",
    "    n_samples = 1_353\n",
    "    n_features = 9\n",
    "    parse_dates = None\n",
    "    converters = {\n",
    "        \"empty_server_form_handler\": float,\n",
    "        \"popup_window\": float,\n",
    "        \"https\": float,\n",
    "        \"request_from_other_domain\": float,\n",
    "        \"anchor_from_other_domain\": float,\n",
    "        \"is_popular\": float,\n",
    "        \"long_url\": float,\n",
    "        \"age_of_domain\": int,\n",
    "        \"ip_in_url\": int,\n",
    "        \"is_phishing\": lambda x: x == \"1\",\n",
    "        }\n",
    "test_size = 0.1\n",
    "dataset, n_samples = data_selector(\n",
    "    data_set=data_set,\n",
    "    filename=filename,\n",
    "    directory=directory,\n",
    "    target=target,\n",
    "    n_features=n_features,\n",
    "    n_samples=n_samples,\n",
    "    converters=converters,\n",
    "    parse_dates=parse_dates,\n",
    ")\n",
    "# target_column is the name of the target column in the resulting data frame df:\n",
    "target_column = \"y\"\n",
    "df = convert_to_df(dataset, target_column=target_column, n_total=n_total)\n",
    "df.columns = [f\"x{i}\" for i in range(1, dataset.n_features + 1)] + [\"y\"]\n",
    "df[\"y\"] = df[\"y\"].astype(int)\n",
    "# update n_samples to the actual number of samples in the data set,\n",
    "# because n_total might be smaller than n_samples which results in a smaller data set:\n",
    "test_size = float(test_size)\n",
    "n_samples = len(df)\n",
    "n_train = int((1.0 - test_size) * n_samples)\n",
    "train = df[:n_train]\n",
    "print(f\"train = {train.shape}\")\n",
    "print(f\"train = {train.head()}\")\n",
    "test = df[n_train:]\n",
    "print(f\"test = {test.shape}\")\n",
    "print(f\"test = {test.tail()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "dataset = datasets.TrumpApproval()\n",
    "target_column = \"Approval\"\n",
    "df = convert_to_df(dataset, target_column)\n",
    "df.rename(columns={\n",
    "    'date': 'ordinal_date',\n",
    "    'Gallup': 'gallup',\n",
    "    'Ipsos': 'ipsos',\n",
    "    'Morning Consult': 'morning_consult',\n",
    "    'Rasmussen': 'rasmussen',\n",
    "    'YouGov': 'you_gov'},\n",
    "    inplace=True)\n",
    "# Split the data into train and test sets\n",
    "train = df[:500]\n",
    "test = df[500:]\n",
    "train.shape, test.shape\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 7) (5, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ordinal_date</th>\n",
       "      <th>gallup</th>\n",
       "      <th>ipsos</th>\n",
       "      <th>morning_consult</th>\n",
       "      <th>rasmussen</th>\n",
       "      <th>you_gov</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>736389</td>\n",
       "      <td>43.843213</td>\n",
       "      <td>46.199250</td>\n",
       "      <td>48.318749</td>\n",
       "      <td>44.104692</td>\n",
       "      <td>43.636914</td>\n",
       "      <td>43.75505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>736390</td>\n",
       "      <td>43.843213</td>\n",
       "      <td>46.199250</td>\n",
       "      <td>48.318749</td>\n",
       "      <td>46.104692</td>\n",
       "      <td>41.636914</td>\n",
       "      <td>43.71027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>736391</td>\n",
       "      <td>43.843213</td>\n",
       "      <td>46.437346</td>\n",
       "      <td>48.318749</td>\n",
       "      <td>47.104692</td>\n",
       "      <td>41.636914</td>\n",
       "      <td>43.90226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>736392</td>\n",
       "      <td>43.843213</td>\n",
       "      <td>46.437346</td>\n",
       "      <td>48.318749</td>\n",
       "      <td>47.104692</td>\n",
       "      <td>41.636914</td>\n",
       "      <td>43.90226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>736393</td>\n",
       "      <td>43.843213</td>\n",
       "      <td>46.437346</td>\n",
       "      <td>48.318749</td>\n",
       "      <td>47.104692</td>\n",
       "      <td>41.636914</td>\n",
       "      <td>43.93693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ordinal_date     gallup      ipsos  morning_consult  rasmussen    you_gov  \\\n",
       "0        736389  43.843213  46.199250        48.318749  44.104692  43.636914   \n",
       "1        736390  43.843213  46.199250        48.318749  46.104692  41.636914   \n",
       "2        736391  43.843213  46.437346        48.318749  47.104692  41.636914   \n",
       "3        736392  43.843213  46.437346        48.318749  47.104692  41.636914   \n",
       "4        736393  43.843213  46.437346        48.318749  47.104692  41.636914   \n",
       "\n",
       "          y  \n",
       "0  43.75505  \n",
       "1  43.71027  \n",
       "2  43.90226  \n",
       "3  43.90226  \n",
       "4  43.93693  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import datasets\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "dataset = datasets.TrumpApproval()\n",
    "df = convert_to_df(dataset, target_column = \"y\", n_total=10)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train = df[:5]\n",
    "test = df[5:]\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "# dataset = datasets.Bananas()\n",
    "dataset = datasets.Phishing()\n",
    "df = convert_to_df(dataset, target_column = \"y\", n_total=10)\n",
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from spotRiver.data.selector import data_selector\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "data_set_values = [\n",
    "    \"Bananas\",\n",
    "    \"Elec2\",\n",
    "    \"HTTP\",\n",
    "    \"Phishing\",\n",
    "]\n",
    "for i in range(len(data_set_values)):\n",
    "    data_set = data_set_values[i]\n",
    "    dataset, n_samples = data_selector(\n",
    "        data_set=data_set\n",
    "    )\n",
    "    df = convert_to_df(dataset, target_column = \"y\", n_total=None)\n",
    "    assert df.shape[0] == n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Phishing websites.\n",
       "\n",
       "This dataset contains features from web pages that are classified as phishing or not.\n",
       "\n",
       "    Name  Phishing                                                                                             \n",
       "    Task  Binary classification                                                                                \n",
       " Samples  1,250                                                                                                \n",
       "Features  9                                                                                                    \n",
       "  Sparse  False                                                                                                \n",
       "    Path  /Users/bartz/miniforge3/envs/spotCondaEnv/lib/python3.11/site-packages/river/datasets/phishing.csv.gz"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import datasets\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'empty_server_form_handler': 0.0,\n",
       "  'popup_window': 0.0,\n",
       "  'https': 0.0,\n",
       "  'request_from_other_domain': 0.0,\n",
       "  'anchor_from_other_domain': 0.0,\n",
       "  'is_popular': 0.5,\n",
       "  'long_url': 1.0,\n",
       "  'age_of_domain': 1,\n",
       "  'ip_in_url': 1},\n",
       " True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dataset))\n",
    "(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model\n",
    "from river import preprocessing\n",
    "\n",
    "model = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LogisticRegression()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': Accuracy: 84.00%, 'Step': 200}\n",
      "{'Accuracy': Accuracy: 86.25%, 'Step': 400}\n",
      "{'Accuracy': Accuracy: 87.50%, 'Step': 600}\n",
      "{'Accuracy': Accuracy: 88.50%, 'Step': 800}\n",
      "{'Accuracy': Accuracy: 89.10%, 'Step': 1000}\n",
      "{'Accuracy': Accuracy: 89.33%, 'Step': 1200}\n",
      "{'Accuracy': Accuracy: 89.28%, 'Step': 1250}\n"
     ]
    }
   ],
   "source": [
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "\n",
    "steps = evaluate.iter_progressive_val_score(\n",
    "    model=model,\n",
    "    dataset=datasets.Phishing(),\n",
    "    metric=metrics.Accuracy(),\n",
    "    step=200\n",
    ")\n",
    "\n",
    "for step in steps:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ROCAUC': ROCAUC: 94.68%, 'Step': 101, 'Prediction': {False: 0.9660278046940564, True: 0.033972195305943614}}\n",
      "{'ROCAUC': ROCAUC: 94.75%, 'Step': 102, 'Prediction': {False: 0.035266280284417584, True: 0.9647337197155824}}\n",
      "{'ROCAUC': ROCAUC: 94.82%, 'Step': 103, 'Prediction': {False: 0.043142536710132906, True: 0.9568574632898671}}\n",
      "{'ROCAUC': ROCAUC: 94.89%, 'Step': 104, 'Prediction': {False: 0.8167366443067448, True: 0.18326335569325528}}\n",
      "{'ROCAUC': ROCAUC: 94.96%, 'Step': 105, 'Prediction': {False: 0.041667261805206746, True: 0.9583327381947933}}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "steps = evaluate.iter_progressive_val_score(\n",
    "    model=model,\n",
    "    dataset=datasets.Phishing(),\n",
    "    metric=metrics.ROCAUC(),\n",
    "    step=1,\n",
    "    yield_predictions=True\n",
    ")\n",
    "\n",
    "for step in itertools.islice(steps, 100, 105):\n",
    "   print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.75, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, auc\n",
    "import numpy as np\n",
    "from spotRiver.evaluation.eval_bml import evaluate_model\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "memory = 0.0\n",
    "r_time = 0.0\n",
    "metric = accuracy_score\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.5, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "metric = cohen_kappa_score\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.8, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "metric = f1_score\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.25, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "metric = hamming_loss\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.75, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import hinge_loss\n",
    "metric = hinge_loss\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.6666666666666666, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "metric = jaccard_score\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.5773502691896258, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "metric = matthews_corrcoef\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.6666666666666666, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "metric = precision_score\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 1.0, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "metric = recall_score\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.75, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "metric = roc_auc_score\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Metric': 0.25, 'Memory (MB)': 0.0, 'CompTime (s)': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "metric = zero_one_loss\n",
    "y_true = np.array([0, 1, 0, 1])\n",
    "y_pred = np.array([0, 1, 1, 1])\n",
    "evaluate_model(y_true, y_pred, memory, r_time, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
