{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Tests\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_bml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spotRiver.evaluation.eval_bml import eval_bml_horizon, eval_bml_landmark, eval_bml_window, eval_oml_horizon\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "metric=mean_absolute_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "df_eval , df_true = eval_bml_horizon(model , train , test , target_column, horizon, metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_bml_landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spotRiver.evaluation.eval_bml import eval_bml_landmark\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "df_eval , df_true = eval_bml_landmark(model , train , test , target_column, horizon, metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_bml_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from spotRiver.evaluation.eval_bml import eval_bml_window\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "df_eval , df_true = eval_bml_window(model , train , test , target_column, horizon, metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_oml_horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model\n",
    "from river import preprocessing,datasets\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "X_train = list(range(10))\n",
    "y_train = [2*x for x in X_train]\n",
    "X_test = list(range(10, 20))\n",
    "y_test = [x for x in X_test]\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\"])\n",
    "train[\"y\"] = y_train\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\"])\n",
    "test[\"y\"] = y_test\n",
    "target_column = \"y\"\n",
    "horizon = 1\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, metric=metric) \n",
    "print(df_eval)\n",
    "print(df_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import linear_model, datasets, preprocessing\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "dataset = datasets.TrumpApproval()\n",
    "target_column = \"Approve\"\n",
    "df = convert_to_df(dataset, target_column)\n",
    "train = df[:500]\n",
    "test = df[500:]\n",
    "horizon = 1\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.evaluation.eval_bml import plot_bml_oml_horizon_metrics, plot_bml_oml_horizon_predictions\n",
    "plot_bml_oml_horizon_metrics(df_eval, metric=metric)\n",
    "plot_bml_oml_horizon_predictions(df_preds, target_column=target_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval_oml_iter_progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from spotRiver.evaluation.eval_oml import eval_oml_iter_progressive, plot_oml_iter_progressive\n",
    "from river import metrics as river_metrics\n",
    "from river import tree as river_tree\n",
    "from river import preprocessing as river_preprocessing\n",
    "dataset = datasets.TrumpApproval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  (river_preprocessing.StandardScaler() | river_tree.HoeffdingAdaptiveTreeRegressor(seed=1))\n",
    "\n",
    "res_num = eval_oml_iter_progressive(\n",
    "    dataset = list(dataset),\n",
    "    step = 1,\n",
    "    metric = river_metrics.MAE(),\n",
    "    models =\n",
    "    {\n",
    "        \"HATR\": model,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_oml_iter_progressive(res_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.arange(100) // 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(100) % 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100 % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.arange(100) // 7\n",
    "# Remove the last two entries\n",
    "rem = 100 % 7\n",
    "arr = arr[:-rem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model, datasets\n",
    "from river import preprocessing\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "dataset = datasets.TrumpApproval()\n",
    "target_column = \"Approval\"\n",
    "df = convert_to_df(dataset, target_column)\n",
    "train = df[:500]\n",
    "test = df[500:]\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "\n",
    "horizon = 10\n",
    "oml_grace_period = 10\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, oml_grace_period, metric=metric)\n",
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the mean of the MAEs of the predicted values and ignore the NaN values\n",
    "df_eval = df_eval.dropna()\n",
    "y = df_eval[\"Metric\"].mean()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1_000_000 / (7*24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "250000/(7*24)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def evaluate_model(y_true: np.ndarray, y_pred: np.ndarray, memory: float, r_time: float, metric) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a model's performance based on its predictions and ground truth values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: Ground truth values as a numpy array.\n",
    "    y_pred: Predicted values as a numpy array.\n",
    "    memory: Memory usage in MB.\n",
    "    r_time: Computation time in seconds.\n",
    "    metric: A function that takes in two arguments (y_true and y_pred) and returns a score.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the evaluation results including the metric score, memory usage and computation time.\n",
    "\n",
    "    Example\n",
    "    ------\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "        import numpy as np\n",
    "        y_true = np.array([1.0, 2.0, 3.0])\n",
    "        y_pred = np.array([1.1, 2.1, 2.9])\n",
    "        memory = 100\n",
    "        r_time = 0.5\n",
    "        result = evaluate_model(y_true=y_true,\n",
    "                                y_pred=y_pred,\n",
    "                                memory=memory,\n",
    "                                r_time=r_time,\n",
    "                                metric=mean_squared_error)\n",
    "        print(result)\n",
    "\n",
    "        # Output:\n",
    "        # {'Metric': 0.00666666666666671, 'Memory (MB)': 100, 'CompTime (s)': 0.5}\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred):\n",
    "        raise ValueError(\"y_true and y_pred must have the same size\")\n",
    "    if (len(y_true) == 0) or (len(y_pred) == 0):\n",
    "        res_dict = {\n",
    "            \"Metric\": None,\n",
    "            \"Memory (MB)\": memory,\n",
    "            \"CompTime (s)\": r_time,\n",
    "        }\n",
    "        return res_dict\n",
    "    score = metric(y_true, y_pred)\n",
    "    res_dict = {\"Metric\": score, \"Memory (MB)\": memory, \"CompTime (s)\": r_time}\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "y_true = np.array([1.0, 2.0, 3.0])\n",
    "y_pred = np.array([1.1, 2.1, 2.9])\n",
    "memory = 100\n",
    "r_time = 0.5\n",
    "result = evaluate_model(y_true=y_true,\n",
    "                        y_pred=y_pred,\n",
    "                        memory=memory,\n",
    "                        r_time=r_time,\n",
    "                        metric=mean_squared_error)\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expect a ValueError:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_model():\n",
    "    y_true = np.array([1.0, 2.0])\n",
    "    y_pred = np.array([1.1, 2.1, 2.9])\n",
    "    memory = 100\n",
    "    r_time = 0.5\n",
    "\n",
    "    try:\n",
    "        result = evaluate_model(y_true=y_true,\n",
    "                                y_pred=y_pred,\n",
    "                                memory=memory,\n",
    "                                r_time=r_time,\n",
    "                                metric=mean_squared_error)\n",
    "    except ValueError as e:\n",
    "        assert str(e) == \"y_true and y_pred must have the same size\"\n",
    "\n",
    "test_evaluate_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class ResourceMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import tracemalloc\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "class ResourceMonitorError(Exception):\n",
    "    pass\n",
    "\n",
    "@dataclass\n",
    "class ResourceUsage:\n",
    "    name: Optional[str]  # Description of Usage\n",
    "    r_time: float  # Measured in seconds\n",
    "    memory: float  # Measured in bytes\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.name is None:\n",
    "            res = [f\"Resource usage for {self.name}:\"]\n",
    "        else:\n",
    "            res = [\"Resource usage:\"]\n",
    "        res.append(f\"  Time [s]: {self.r_time}\")\n",
    "        res.append(f\"  Memory [b]: {self.memory}\")\n",
    "        return \"\\n\".join(res)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self, name: Optional[str] = None):\n",
    "        self.name = name\n",
    "        self.r_time = None\n",
    "        self.memory = None\n",
    "        self.current_memory = None\n",
    "        self.peak_memory = None\n",
    "        self._start = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        if tracemalloc.is_tracing():\n",
    "            raise ResourceMonitorError(\"Already tracing memory usage!\")\n",
    "        tracemalloc.start()\n",
    "        tracemalloc.reset_peak()\n",
    "        self._start = time.perf_counter_ns()\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.r_time = (time.perf_counter_ns() - self._start) / 1.0e9\n",
    "        _, peak = tracemalloc.get_traced_memory()\n",
    "        self.memory = peak / (1024 * 1024)\n",
    "        tracemalloc.stop()\n",
    "\n",
    "    def result(self):\n",
    "        if self.r_time is None or self.memory is None:\n",
    "            raise ResourceMonitorError(\"No resources monitored yet.\")\n",
    "        return ResourceUsage(name=self.name, r_time=self.r_time, memory=self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = ResourceMonitor()\n",
    "with rm:\n",
    "    x = 10 ** 6\n",
    "print(rm.result())\n",
    "# Output:\n",
    "# Resource usage for None:\n",
    "#   Time [s]: 2.917e-06\n",
    "#   Memory [b]: 8.7738037109375e-05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval_bml Horizon Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def eval_bml_horizon_default(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    for batch_number, batch_df in test.groupby(np.arange(len(test)) // horizon):\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            preds = pd.Series(model.predict(batch_df.loc[:, batch_df.columns != target_column]))\n",
    "        diffs = batch_df[target_column].values - preds\n",
    "        df_eval.loc[batch_number + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=batch_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval BML New:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bml_horizon(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    # Reset index of train and test dataframes\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    # Initialize lists for predictions and differences\n",
    "    preds_list = []\n",
    "    diffs_list = []\n",
    "    # Fit the model on the training data\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    # Evaluate the model on empty arrays to get initial resource usage\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    # If include_remainder is False, remove remainder rows from test dataframe\n",
    "    if include_remainder is False:\n",
    "        remainder = len(test) % horizon\n",
    "        if remainder > 0:\n",
    "            test = test[:-remainder]\n",
    "    # Evaluate the model on batches of size horizon from the test dataframe\n",
    "    for batch_number, batch_df in test.groupby(np.arange(len(test)) // horizon):\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            preds = model.predict(batch_df.loc[:, batch_df.columns != target_column])\n",
    "        diffs = batch_df[target_column].values - preds\n",
    "        df_eval.loc[batch_number + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=batch_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        # Append predictions and differences to their respective lists\n",
    "        preds_list.append(preds)\n",
    "        diffs_list.append(diffs)\n",
    "\n",
    "    # Concatenate predictions and differences lists into series\n",
    "    series_preds = pd.Series(np.concatenate(preds_list))\n",
    "    series_diffs = pd.Series(np.concatenate(diffs_list))\n",
    "\n",
    "    # Create a dataframe with true values and add columns for predictions and differences\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Regresssion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create synthetic data for regression with 100 observations, 3 features and one target value\n",
    "X_train, y_train = make_regression(n_samples=80, n_features=3, n_targets=1)\n",
    "X_test, y_test = make_regression(n_samples=20, n_features=3, n_targets=1)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = mean_absolute_error\n",
    "df_eval, df_true = eval_bml_horizon(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Create synthetic data for classification with 10 observations and one target value\n",
    "X_train, y_train = make_classification(n_samples=10, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=2, random_state=0)\n",
    "X_test, y_test = make_classification(n_samples=10, n_features=2, n_informative=2,\n",
    "                           n_redundant=0, n_classes=2, random_state=2)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=['Feature 1', 'Feature 2'])\n",
    "train['y'] = y_train\n",
    "test = pd.DataFrame(X_test, columns=['Feature 1', 'Feature 2'])\n",
    "test['y'] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = accuracy_score\n",
    "df_eval, df_true = eval_bml_horizon(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2nd example classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X_train, y_train = make_classification(n_samples=1000)\n",
    "X_test, y_test = make_classification(n_samples=1000)\n",
    "\n",
    "# Convert to DataFrames and add target column\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['target'] = y_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['target'] = y_test\n",
    "\n",
    "# Initialize Decision Tree Classifier and evaluate using eval_bml_horizon function\n",
    "dtc = DecisionTreeClassifier()\n",
    "eval_results_df, true_preds_diffs_df = eval_bml_horizon(dtc, train_df,test_df,'target', 10, True, accuracy_score)\n",
    "\n",
    "# Print first 5 rows of evaluation results DataFrame\n",
    "print(eval_results_df.head())\n",
    "\n",
    "# Print first 5 rows of true values/predictions/differences DataFrame\n",
    "print(true_preds_diffs_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BML Landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Generator\n",
    "def gen_sliding_window(\n",
    "    df: pd.DataFrame, horizon: int, include_remainder: bool = True\n",
    ") -> Generator[pd.DataFrame, None, None]:\n",
    "    i = 0\n",
    "    while True:\n",
    "        subset = df[i * horizon : (i + 1) * horizon]\n",
    "        if len(subset) == 0:\n",
    "            break\n",
    "        elif len(subset) < horizon:\n",
    "            if include_remainder:\n",
    "                yield subset\n",
    "            break\n",
    "        i += 1\n",
    "        yield subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bml_landmark(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    # Landmark Evaluation\n",
    "    for i, new_df in enumerate(gen_sliding_window(test, horizon)):\n",
    "        train = pd.concat([train, new_df], ignore_index=True)\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            preds = pd.Series(model.predict(new_df.loc[:, new_df.columns != target_column]))\n",
    "            model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "\n",
    "        diffs = new_df[target_column].values - preds\n",
    "        df_eval.loc[i + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=new_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create synthetic data for regression with 100 observations, 3 features and one target value\n",
    "X_train, y_train = make_regression(n_samples=80, n_features=3, n_targets=1)\n",
    "X_test, y_test = make_regression(n_samples=20, n_features=3, n_targets=1)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = mean_absolute_error\n",
    "df_eval, df_true = eval_bml_landmark(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X_train, y_train = make_classification(n_samples=1000)\n",
    "X_test, y_test = make_classification(n_samples=1000)\n",
    "\n",
    "# Convert to DataFrames and add target column\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['target'] = y_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['target'] = y_test\n",
    "\n",
    "# Initialize Decision Tree Classifier and evaluate using eval_bml_landmark function \n",
    "dtc = DecisionTreeClassifier()\n",
    "eval_results_df, true_preds_diffs_df = eval_bml_landmark(dtc, train_df,test_df,'target', 100, True, accuracy_score)\n",
    "\n",
    "# Print first 5 rows of evaluation results DataFrame \n",
    "print(eval_results_df.head())\n",
    "\n",
    "# Print first 5 rows of true values/predictions/differences DataFrame \n",
    "print(true_preds_diffs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval BML Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_horizon_shifted_window(df, window_size, horizon):\n",
    "    i = 0\n",
    "    while True:\n",
    "        train_window = df[i * horizon : i * horizon + window_size]\n",
    "        test_window = df[i * horizon + window_size : (i + 1) * horizon + window_size]\n",
    "        if len(test_window) == 0:\n",
    "            break\n",
    "        elif len(test_window) < horizon:\n",
    "            yield train_window, test_window\n",
    "            break\n",
    "        i += 1\n",
    "        yield train_window, test_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bml_window(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    ") -> tuple:\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    df_all = pd.concat([train, test], ignore_index=True)\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        model.fit(train.loc[:, train.columns != target_column], train[target_column])\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    for i, (w_train, w_test) in enumerate(gen_horizon_shifted_window(df_all, len(train), horizon)):\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            model.fit(w_train.loc[:, w_train.columns != target_column], w_train[target_column])\n",
    "            preds = pd.Series(model.predict(w_test.loc[:, w_test.columns != target_column]))\n",
    "\n",
    "        diffs = w_test[target_column].values - preds\n",
    "        df_eval.loc[i + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=w_test[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create synthetic data for regression with 100 observations, 3 features and one target value\n",
    "X_train, y_train = make_regression(n_samples=80, n_features=3, n_targets=1)\n",
    "X_test, y_test = make_regression(n_samples=20, n_features=3, n_targets=1)\n",
    "\n",
    "# Convert the data into a pandas data frame\n",
    "train = pd.DataFrame(X_train, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "train[\"y\"] = y_train\n",
    "\n",
    "test = pd.DataFrame(X_test, columns=[\"x1\", \"x2\", \"x3\"])\n",
    "test[\"y\"] = y_test\n",
    "\n",
    "target_column = \"y\"\n",
    "horizon = 2\n",
    "include_remainder = False\n",
    "metric = mean_absolute_error\n",
    "df_eval, df_true = eval_bml_window(model , train , test , target_column, horizon, include_remainder = include_remainder , metric=metric)\n",
    "print (df_eval )\n",
    "print (df_true )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate synthetic data for classification\n",
    "X_train, y_train = make_classification(n_samples=1000)\n",
    "X_test, y_test = make_classification(n_samples=1000)\n",
    "\n",
    "# Convert to DataFrames and add target column\n",
    "train_df = pd.DataFrame(X_train)\n",
    "train_df['target'] = y_train\n",
    "\n",
    "test_df = pd.DataFrame(X_test)\n",
    "test_df['target'] = y_test\n",
    "\n",
    "# Initialize Decision Tree Classifier and evaluate using eval_bml_landmark function \n",
    "dtc = DecisionTreeClassifier()\n",
    "eval_results_df, true_preds_diffs_df = eval_bml_window(dtc, train_df,test_df,'target', 100, True, accuracy_score)\n",
    "\n",
    "# Print first 5 rows of evaluation results DataFrame \n",
    "print(eval_results_df.head())\n",
    "\n",
    "# Print first 5 rows of true values/predictions/differences DataFrame \n",
    "print(true_preds_diffs_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval OML Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import stream as river_stream\n",
    "def eval_oml_horizon(\n",
    "    model: object,\n",
    "    train: pd.DataFrame,\n",
    "    test: pd.DataFrame,\n",
    "    target_column: str,\n",
    "    horizon: int,\n",
    "    include_remainder: bool = True,\n",
    "    metric: object = None,\n",
    "    oml_grace_period: int = None,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    # Check if metric is None or null and raise ValueError if it is\n",
    "    if metric is None:\n",
    "        raise ValueError(\"The 'metric' parameter must not be None or null.\")\n",
    "    if oml_grace_period is None:\n",
    "        oml_grace_period = horizon\n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    if include_remainder is False:\n",
    "        rem = len(test) % horizon\n",
    "        if rem > 0:\n",
    "            test = test[:-rem]\n",
    "    series_preds = pd.Series(dtype=float)\n",
    "    series_diffs = pd.Series(dtype=float)\n",
    "\n",
    "    # Initial Training on Train Data\n",
    "    # For OML, this is performed on a limited subset only (oml_grace_period).\n",
    "    train_X = train.loc[:, train.columns != target_column]\n",
    "    train_y = train[target_column]\n",
    "    train_X = train_X.tail(oml_grace_period)\n",
    "    train_y = train_y.tail(oml_grace_period)\n",
    "    rm = ResourceMonitor()\n",
    "    with rm:\n",
    "        for xi, yi in river_stream.iter_pandas(train_X, train_y):\n",
    "            # The following line returns y_pred, which is not used, therefore set to \"_\":\n",
    "            _ = model.predict_one(xi)\n",
    "            # metric = metric.update(yi, y_pred)\n",
    "            model = model.learn_one(xi, yi)\n",
    "    df_eval = pd.DataFrame.from_dict(\n",
    "        [evaluate_model(y_true=np.array([]), y_pred=np.array([]), memory=rm.memory, r_time=rm.r_time, metric=metric)]\n",
    "    )\n",
    "\n",
    "    # Test Data Evaluation\n",
    "    for i, new_df in enumerate(gen_sliding_window(test, horizon)):\n",
    "        preds = []\n",
    "        test_X = new_df.loc[:, new_df.columns != target_column]\n",
    "        test_y = new_df[target_column]\n",
    "        rm = ResourceMonitor()\n",
    "        with rm:\n",
    "            for xi, yi in river_stream.iter_pandas(test_X, test_y):\n",
    "                pred = model.predict_one(xi)\n",
    "                preds.append(pred)  # This is falsly measured with the ResourceMonitor\n",
    "                model = model.learn_one(xi, yi)\n",
    "        preds = pd.Series(preds)\n",
    "        diffs = new_df[target_column].values - preds\n",
    "        df_eval.loc[i + 1] = pd.Series(\n",
    "            evaluate_model(\n",
    "                y_true=new_df[target_column],\n",
    "                y_pred=preds,\n",
    "                memory=rm.memory,\n",
    "                r_time=rm.r_time,\n",
    "                metric=metric,\n",
    "            )\n",
    "        )\n",
    "        series_preds = pd.concat([series_preds, preds], ignore_index=True)\n",
    "        series_diffs = pd.concat([series_diffs, diffs], ignore_index=True)\n",
    "    df_true = pd.DataFrame(test[target_column])\n",
    "    df_true[\"Prediction\"] = series_preds\n",
    "    df_true[\"Difference\"] = series_diffs\n",
    "    return df_eval, df_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model, preprocessing, datasets\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "from river import datasets\n",
    "import pandas as pd\n",
    "dataset = datasets.TrumpApproval()\n",
    "data_dict = {key: [] for key in list(dataset)[0][0].keys()}\n",
    "data_dict[\"Approval\"] = []\n",
    "for x in dataset:\n",
    "    for key, value in x[0].items():\n",
    "        data_dict[key].append(value)\n",
    "    data_dict[\"Approval\"].append(x[1])\n",
    "df = pd.DataFrame(data_dict)\n",
    "df.rename(columns={'date': 'ordinal_date', 'Gallup': 'gallup', 'Ipsos': 'ipsos', 'Morning Consult': 'morning_consult', 'Rasmussen': 'rasmussen', 'YouGov': 'you_gov'}, inplace=True)\n",
    "train = df[0:499]\n",
    "test = df[500:-1]\n",
    "target_column = \"Approval\"\n",
    "horizon = 10\n",
    "oml_grace_period = 5\n",
    "include_remainder = True\n",
    "df_eval, df_preds = eval_oml_horizon(model=model,\n",
    "                                     train=train,\n",
    "                                     test=test,\n",
    "                                     target_column=target_column,\n",
    "                                     horizon=horizon,\n",
    "                                     include_remainder=include_remainder,\n",
    "                                     metric= mean_absolute_error,\n",
    "                                     oml_grace_period=oml_grace_period)\n",
    "df_eval.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 7*24\n",
    "k = 0.1\n",
    "n_total = int(k*100_000)\n",
    "p_1 = int(k*25_000)\n",
    "p_2 = int(k*50_000)\n",
    "position=(p_1, p_2)\n",
    "n_train = 1_000\n",
    "a = n_train + p_1 - 12\n",
    "b = a + 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "dataset = synth.FriedmanDrift(\n",
    "   drift_type='gra',\n",
    "   position=position,\n",
    "     seed=123\n",
    ")\n",
    "data_dict = {key: [] for key in list(dataset.take(1))[0][0].keys()}\n",
    "data_dict[\"y\"] = []\n",
    "for x, y in dataset.take(n_total):\n",
    "    for key, value in x.items():\n",
    "        data_dict[key].append(value)\n",
    "    data_dict[\"y\"].append(y)\n",
    "df = pd.DataFrame(data_dict)\n",
    "# Add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column\n",
    "df.columns = [f\"x{i}\" for i in range(1, 11)] + [\"y\"]\n",
    "train = df[:n_train]\n",
    "test = df[n_train:]\n",
    "target_column = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_lm = preprocessing.StandardScaler()\n",
    "oml_lm |= linear_model.LinearRegression()\n",
    "\n",
    "df_eval_oml_lm, df_true_oml_lm = eval_oml_horizon(model=oml_lm, train=train, test=test, target_column=\"y\", horizon=horizon, metric=mean_absolute_error, oml_grace_period=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_oml_lm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_oml_lm.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.datasets import synth\n",
    "dataset = synth.SEA(variant=0, seed=42)\n",
    "data_dict = {key: [] for key in list(dataset.take(1))[0][0].keys()}\n",
    "data_dict[\"y\"] = []\n",
    "for x, y in dataset.take(n_total):\n",
    "    for key, value in x.items():\n",
    "        data_dict[key].append(value)\n",
    "    data_dict[\"y\"].append(y)\n",
    "df = pd.DataFrame(data_dict)\n",
    "# Add column names x1 until x10 to the first 10 columns of the dataframe and the column name y to the last column\n",
    "df.columns = [f\"x{i}\" for i in range(1, 4)] + [\"y\"]\n",
    "df = df.apply(lambda x: x.astype(int) if x.dtype == bool else x)\n",
    "train = df[:n_train]\n",
    "test = df[n_train:]\n",
    "target_column = \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oml_lm = preprocessing.StandardScaler()\n",
    "oml_lm |= linear_model.LogisticRegression()\n",
    "\n",
    "df_eval_oml_lm, df_true_oml_lm = eval_oml_horizon(model=oml_lm, train=train, test=test, target_column=\"y\", horizon=horizon, metric=accuracy_score, oml_grace_period=horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_oml_lm.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_bml_oml_horizon_metrics(\n",
    "    df_eval: list[pd.DataFrame] = None,\n",
    "    df_labels: list = None,\n",
    "    log_x=False,\n",
    "    log_y=False,\n",
    "    cumulative=True,\n",
    "    grid=True,\n",
    "    fig_width=16,\n",
    "    fig_height=5,\n",
    "    metric=None,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    # Check if input dataframes are provided\n",
    "    if df_eval is not None:\n",
    "        df_list = copy.deepcopy(df_eval)\n",
    "        # Convert single dataframe input to a list if needed\n",
    "        if df_list.__class__ != list:\n",
    "            df_list = [df_list]\n",
    "        # Define metric names and titles\n",
    "        metric_name = metric.__class__.__name__\n",
    "        metrics = [\"Metric\", \"CompTime (s)\", \"Memory (MB)\"]\n",
    "        titles = [metric_name, \"Computation time (s)\", \"Memory (MB)\"]\n",
    "        # Create subplots with shared x-axis\n",
    "        fig, axes = plt.subplots(3, figsize=(fig_width, fig_height), constrained_layout=True, sharex=True)\n",
    "        # Loop over each dataframe in input list\n",
    "        for j, df in enumerate(df_list):\n",
    "            if cumulative:\n",
    "                # df.MAE = np.cumsum(df.MAE) / range(1, (1 + df.MAE.size))\n",
    "                df[\"Metric\"] = np.cumsum(df[\"Metric\"]) / range(1, (1 + df[\"Metric\"].size))\n",
    "                df[\"CompTime (s)\"] = np.cumsum(df[\"CompTime (s)\"])  # / range(1, (1 + df[\"CompTime (s)\"].size))\n",
    "                # df[\"Memory (MB)\"] = np.cumsum(df[\"Memory (MB)\"]) / range(1, (1 + df[\"Memory (MB)\"].size))\n",
    "            # Loop over each metric\n",
    "            for i in range(3):\n",
    "                # Assign label based on input or default value\n",
    "                if df_labels is None:\n",
    "                    label = f\"{j}\"\n",
    "                else:\n",
    "                    label = df_labels[j]\n",
    "                # Plot metric values against dataset names\n",
    "                axes[i].plot(df.index.values.tolist(), df[metrics[i]].values.tolist(), label=label, **kwargs)\n",
    "                # Set title and legend\n",
    "                axes[i].set_title(titles[i])\n",
    "                axes[i].legend(loc=\"upper right\")\n",
    "                axes[i].grid(grid)\n",
    "                # Set logarithmic scales if specified\n",
    "                if log_x:\n",
    "                    axes[i].set_xscale(\"log\")\n",
    "                if log_y:\n",
    "                    axes[i].set_yscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels=[\"oml\"]\n",
    "plot_bml_oml_horizon_metrics(df_eval = [df_eval_oml_lm], df_labels=df_labels, cumulative=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true_oml_lm = df_true_oml_lm.apply(lambda x: x.astype(int) if x.dtype == bool else x)\n",
    "df_true_oml_lm.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bml_oml_horizon_predictions(\n",
    "df_true: list[pd.DataFrame] = None,\n",
    "df_labels: list = None,\n",
    "target_column: str = \"Actual\",\n",
    "log_x=False,\n",
    "log_y=False,\n",
    "skip_first_n=0,\n",
    "grid=True,\n",
    "fig_width=16,\n",
    "fig_height=5,\n",
    "**kwargs,\n",
    ") -> None:\n",
    "    if df_true is not None:\n",
    "        df_plot = copy.deepcopy(df_true)\n",
    "    if df_plot.__class__ != list:\n",
    "        df_plot = [df_plot]\n",
    "    # plot actual vs predicted values\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    for j, df in enumerate(df_plot):\n",
    "        # Assign label based on input or default value\n",
    "        if df_labels is None:\n",
    "            label = f\"{j}\"\n",
    "        else:\n",
    "            label = df_labels[j]\n",
    "        # skip first n values\n",
    "        df[\"Prediction\"][range(skip_first_n)] = np.nan\n",
    "        plt.plot(df.index, df[\"Prediction\"], label=label, **kwargs)\n",
    "    # Plot the actual value only once:\n",
    "    plt.plot(df_plot[0].index, df_plot[0][target_column], label=\"Actual\", **kwargs)\n",
    "    plt.title(\"Actual vs Prediction\")\n",
    "    if log_x:\n",
    "        plt.xscale(\"log\")\n",
    "    if log_y:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.grid(grid)\n",
    "    plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bml_oml_horizon_predictions(df_true = [df_true_oml_lm[0:150]], target_column=target_column,  df_labels=df_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = {'abc': [1, 2, 3], 'B': [4, 5, 6]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Set the first n rows in column \"abc\" to np.nan\n",
    "n = 1\n",
    "df.loc[:n-1,'abc'] = np.nan\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bml_oml_horizon_predictions(\n",
    "    df_true: list[pd.DataFrame] = None,\n",
    "    df_labels: list = None,\n",
    "    target_column: str = \"Actual\",\n",
    "    log_x=False,\n",
    "    log_y=False,\n",
    "    skip_first_n=0,\n",
    "    grid=True,\n",
    "    fig_width=16,\n",
    "    fig_height=5,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    if df_true is not None:\n",
    "        df_plot = copy.deepcopy(df_true)\n",
    "        if df_plot.__class__ != list:\n",
    "            df_plot = [df_plot]\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        for j, df in enumerate(df_plot):\n",
    "            if df_labels is None:\n",
    "                label = f\"{j}\"\n",
    "            else:\n",
    "                label = df_labels[j]\n",
    "            df.loc[: skip_first_n - 1, \"Prediction\"] = np.nan\n",
    "            plt.plot(df.index, df[\"Prediction\"], label=label, **kwargs)\n",
    "        plt.plot(df_plot[0].index, df_plot[0][target_column], label=\"Actual\", **kwargs)\n",
    "        plt.title(\"Actual vs Prediction\")\n",
    "        if log_x:\n",
    "            plt.xscale(\"log\")\n",
    "        if log_y:\n",
    "            plt.yscale(\"log\")\n",
    "        plt.grid(grid)\n",
    "        plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "import math\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from math import inf\n",
    "from spotPython.spot import spot\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "from spotRiver.utils.selectors import select_leaf_prediction, select_leaf_model\n",
    "from spotRiver import data\n",
    "from scipy.optimize import differential_evolution\n",
    "import matplotlib.pyplot as plt\n",
    "from river import datasets, time_series, utils, compose, linear_model, optim, preprocessing, evaluate, metrics, tree \n",
    "from river.datasets import synth\n",
    "import copy\n",
    "import warnings\n",
    "import numbers\n",
    "from sklearn.preprocessing import OneHotEncoder , MinMaxScaler\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.pipeline import make_pipeline , Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bike Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotRiver.data.bike_sharing import get_bike_sharing_data\n",
    "import river.stream as river_stream\n",
    "import copy\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "df, train, test = get_bike_sharing_data()\n",
    "target_column=\"count\"\n",
    "n_samples = df.shape[0]\n",
    "X = copy.deepcopy(train)\n",
    "y = X.pop(\"count\")\n",
    "data = river_stream.iter_pandas(X, y)\n",
    "dataset = list(data)\n",
    "#\n",
    "categorical_columns = [\n",
    "    \"weather\",\n",
    "    \"season\",\n",
    "    \"holiday\",\n",
    "    \"workingday\",\n",
    "]\n",
    "categories = [\n",
    "    [\"clear\", \"misty\", \"rain\"],\n",
    "    [\"spring\", \"summer\", \"fall\", \"winter\"],\n",
    "    [\"False\", \"True\"],\n",
    "    [\"False\", \"True\"],\n",
    "]\n",
    "\n",
    "m = test.shape[0]\n",
    "a = int(m/2)-100\n",
    "b = int(m/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trump Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error as mae\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from river import linear_model\n",
    "# from river import preprocessing, datasets\n",
    "# from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "# from river import datasets\n",
    "# import pandas as pd\n",
    "# from spotRiver.utils.data_conversion import convert_to_df\n",
    "# dataset = datasets.TrumpApproval()\n",
    "# target_column = \"Approval\"\n",
    "# df = convert_to_df(dataset, target_column)\n",
    "# df.rename(columns={'date': 'ordinal_date', 'Gallup': 'gallup', 'Ipsos': 'ipsos', 'Morning Consult': 'morning_consult', 'Rasmussen': 'rasmussen', 'YouGov': 'you_gov'}, inplace=True)\n",
    "# train = df[0:499]\n",
    "# test = df[500:-1]\n",
    "## model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river.tree.splitter import EBSTSplitter, QOSplitter, TEBSTSplitter, GaussianSplitter, HistogramSplitter\n",
    "from river.linear_model import LinearRegression, PARegressor, Perceptron\n",
    "from river.tree import HoeffdingTreeRegressor\n",
    "from river.preprocessing import StandardScaler\n",
    "from river.compose import Pipeline\n",
    "from numpy import power\n",
    "from spotPython.utils.convert import class_for_name\n",
    "import json\n",
    "import numpy as np\n",
    "from spotRiver.data.river_hyper_dict import HyperDict\n",
    "from spotRiver.utils.assignments import assign_values, iterate_dict_values, convert_keys\n",
    "fun_control = {}\n",
    "# default hyper_dict for all algorithms, hyperparameters and levels\n",
    "# river_hyper_dict = HyperDict().load()\n",
    "# Load loacal hyper_dict:\n",
    "with open(\"river_hyper_dict.json\", \"r\") as f:\n",
    "         river_hyper_dict = json.load(f)\n",
    "core_model = HoeffdingTreeRegressor\n",
    "fun_control.update({\"core_model\": core_model})\n",
    "algorithm = \"HoeffdingTreeRegressor\"\n",
    "algorithm_hyper_dict = river_hyper_dict[algorithm]\n",
    "hyperparameter = \"leaf_prediction\"\n",
    "levels = [\"mean\", \"model\", \"adaptive\"]\n",
    "algorithm_hyper_dict[hyperparameter].update({\"levels\": levels})\n",
    "lower = 0\n",
    "upper = len(algorithm_hyper_dict[hyperparameter]) - 1\n",
    "fun_control.update({\"algorithm_hyper_dict\": algorithm_hyper_dict})\n",
    "fun_control.update({\"core_model_name\": algorithm})\n",
    "prep_model = StandardScaler()\n",
    "fun_control.update({\"prep_model\": prep_model})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: generate lower and upper bounds for each hyperparameter\n",
    "# fun_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function that takes the dictionary d and a key and returns the length of the \"levels\" list of the key. If the key is not in the dictionary, throw a value error.\n",
    "def get_upper_bound(d, key):\n",
    "    if key in d:\n",
    "        return len(d[key][\"levels\"]) - 1\n",
    "    else:\n",
    "        raise ValueError(\"Key not in dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function that takes the dictionaries d and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from d. If the key value in the dictionary is 0, then take the first value from the list, if it is 1, take the second and so on. If a key is not in d, throw a value error. For example, if d= {\"HoeffdingTreeRegressor\": { \"leaf_prediction\": {\"levels\": [\"mean\", \"model\", \"adaptive\"],                           \"type\": \"factor\",   \"default\": \"mean\",                            \"river_parameter_type\": \"str\"},        \"leaf_model\": {\"levels\": [\"LinearRegression\", \"PARegressor\", \"Perceptron\"],                        \"type\": \"factor\",                        \"default\": \"LinearRegression\", \"river_parameter_type\": \"instance\"},        \"splitter\": {\"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],     \"type\": \"factor\",\"default\": \"EBSTSplitter\",                        \"river_parameter_type\": \"instance()\"},        \"binary_split\": {\"levels\": [0, 1],                        \"type\": \"factor\",                        \"default\": 0,                        \"river_parameter_type\": \"bool\"},        \"stop_mem_management\": {\"levels\": [0, 1],                                \"type\": \"factor\",                                \"default\": 0,                                \"river_parameter_type\": \"bool\"}    }} and v is {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 0, 'leaf_model': 0, 'model_selector_decay': 0.95, 'splitter': 1, 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0} then the function should return {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': 'LinearRegression', 'model_selector_decay': 0.95, 'splitter': 'TEBSTSplitter', 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0}\n",
    "# def get_dict_with_levels(d, v):\n",
    "#     new_dict = {}\n",
    "#     for key, value in v.items():\n",
    "#         if key in d:\n",
    "#             new_dict[key] = d[key][\"levels\"][value]\n",
    "#         else:\n",
    "#             new_dict[key] = v[key]\n",
    "#     return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function that takes the dictionaries d and v and returns a new dictionary with the same keys as v but with the values of the levels of the keys from d. If the key value in the dictionary is 0, then take the first value from the list, if it is 1, take the second and so on. If a key is not in d, take the key from v. If the river_parameter_type value is instance, return the class of the value from the module river via getattr(\"river\", value). For example, if d= {\"HoeffdingTreeRegressor\": { \"leaf_prediction\": {\"levels\": [\"mean\", \"model\", \"adaptive\"],                           \"type\": \"factor\",   \"default\": \"mean\",                            \"river_parameter_type\": \"str\"},        \"leaf_model\": {\"levels\": [\"linear_model.LinearRegression\", \"linear_model.PARegressor\", \"linear_model.Perceptron\"],                        \"type\": \"factor\",                        \"default\": \"LinearRegression\", \"river_parameter_type\": \"instance\"},        \"splitter\": {\"levels\": [\"EBSTSplitter\", \"TEBSTSplitter\", \"QOSplitter\"],     \"type\": \"factor\",\"default\": \"EBSTSplitter\",                        \"river_parameter_type\": \"instance()\"},        \"binary_split\": {\"levels\": [0, 1],                        \"type\": \"factor\",                        \"default\": 0,                        \"river_parameter_type\": \"bool\"},        \"stop_mem_management\": {\"levels\": [0, 1],                                \"type\": \"factor\",                                \"default\": 0,                                \"river_parameter_type\": \"bool\"}    }} and v is {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 0, 'leaf_model': 0, 'model_selector_decay': 0.95, 'splitter': 1, 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0} then the function should return {'grace_period': 200, 'max_depth': 10, 'delta': 1e-07, 'tau': 0.05, 'leaf_prediction': 'mean', 'leaf_model': linear_model.LinearRegression, 'model_selector_decay': 0.95, 'splitter': 'TEBSTSplitter', 'min_samples_split': 9, 'binary_split': 0, 'max_size': 500.0}.   Note that the value of the key \"leaf_model\" is a class and not a string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "\n",
    "\n",
    "# def class_for_name(module_name, class_name) -> object:\n",
    "#     \"\"\"Returns a class for a given module and class name.\n",
    "\n",
    "#     Parameters:\n",
    "#         module_name (str): The name of the module.\n",
    "#         class_name (str): The name of the class.\n",
    "\n",
    "#     Returns:\n",
    "#         object: The class.\n",
    "\n",
    "#     Example:\n",
    "#         >>> from spotPython.utils.convert import class_for_name\n",
    "#             from scipy.optimize import rosen\n",
    "#             bounds = [(0,2), (0, 2), (0, 2), (0, 2), (0, 2)]\n",
    "#             shgo_class = class_for_name(\"scipy.optimize\", \"shgo\")\n",
    "#             result = shgo_class(rosen, bounds)\n",
    "#     \"\"\"\n",
    "#     m = importlib.import_module(module_name)\n",
    "#     print(m)\n",
    "#     c = getattr(m, class_name)\n",
    "#     print(c)\n",
    "#     return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spotPython.utils.convert import class_for_name\n",
    "def get_dict_with_levels_and_types(d, v):\n",
    "    new_dict = {}\n",
    "    for key, value in v.items():\n",
    "        if key in d and d[key][\"type\"] == \"factor\":\n",
    "            if d[key][\"river_parameter_type\"] == \"instance\":\n",
    "                print(d[key][\"levels\"][value])\n",
    "                # remove the last part of [\"a.b.c\"] and unlist it. For example, [\"a.b.c\"] becomes \"a.b\"\n",
    "                # mdl = \".\".join(d[key][\"levels\"][value].split(\".\")[:-1])\n",
    "                # if d[key][\"class_name\"] exists, use it, otherwise skip the command\n",
    "                if \"class_name\" in d[key]:\n",
    "                    mdl = d[key][\"class_name\"]\n",
    "                print(\"mdl\", mdl)\n",
    "                # select the last part of the string \"a.b.c\"\n",
    "                ## c = d[key][\"levels\"][value].split(\".\")[-1]\n",
    "                c = d[key][\"levels\"][value]\n",
    "                print(\"c\", c)\n",
    "                new_dict[key] = class_for_name(mdl, c)\n",
    "            elif d[key][\"river_parameter_type\"] == \"instance()\":\n",
    "                # mdl = \".\".join(d[key][\"levels\"][value].split(\".\")[:-1])\n",
    "                mdl = d[key][\"class_name\"]\n",
    "                print(mdl)\n",
    "                # select the last part of the string \"a.b.c\"\n",
    "                # c = d[key][\"levels\"][value].split(\".\")[-1]\n",
    "                c = d[key][\"levels\"][value]\n",
    "                print(c)\n",
    "                k = class_for_name(mdl, c)\n",
    "                new_dict[key] = k()\n",
    "            else:\n",
    "                new_dict[key] = d[key][\"levels\"][value]\n",
    "        else:\n",
    "            new_dict[key] = v[key]\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the values from the \"default\" keys  from the dictionary fun_control[\"algorithm_hyper_dict\"] as a list. if the key of the value has as \"type\" the value \"int\" or \"float\", convert the value to the corresponding type  \n",
    "def get_default_values_from_dict(d):\n",
    "    new_dict = {}\n",
    "    for key, value in d.items():\n",
    "        if value[\"type\"] == \"int\":\n",
    "            new_dict[key] = int(value[\"default\"])\n",
    "        elif value[\"type\"] == \"float\":\n",
    "            new_dict[key] = float(value[\"default\"])\n",
    "        else:\n",
    "            new_dict[key] = value[\"default\"]\n",
    "    return new_dict\n",
    "default_dict = get_default_values_from_dict(fun_control[\"algorithm_hyper_dict\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[fun_control[\"algorithm_hyper_dict\"][key][\"default\"] for key in fun_control[\"algorithm_hyper_dict\"].keys()]])\n",
    "# X.shape[1]\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fun_control.update({\"var_name\": list(fun_control[\"algorithm_hyper_dict\"].keys()),\n",
    "#            \"var_type\": list(fun_control[\"algorithm_hyper_dict\"][key][\"type\"] for key in fun_control[\"algorithm_hyper_dict\"].keys())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_type = list(fun_control[\"algorithm_hyper_dict\"][key][\"type\"] for key in fun_control[\"algorithm_hyper_dict\"].keys())\n",
    "var_name = list(fun_control[\"algorithm_hyper_dict\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import metrics\n",
    "n_samples = 500 # Trump train and test\n",
    "horizon = 7*24\n",
    "oml_grace_period = 2\n",
    "fun = HyperRiver(seed=123, log_level=50).fun_oml_horizon\n",
    "fun_control.update({\"data\": None, # dataset,\n",
    "               \"train\": train,\n",
    "               \"test\": test,\n",
    "               \"target_column\": target_column,\n",
    "               \"horizon\": horizon,\n",
    "               \"oml_grace_period\": oml_grace_period,\n",
    "               \"n_samples\": n_samples,\n",
    "               \"weights\": np.array([1, 1/1000, 1/1000])*10_000.0,\n",
    "               \"step\": 100,\n",
    "               \"log_level\": 50,\n",
    "               \"weight_coeff\": 1.0,\n",
    "               \"metric\": metrics.MAE(),\n",
    "               \"var_name\": var_name,\n",
    "               \"var_type\": var_type,\n",
    "               \"prep_model\": prep_model,\n",
    "               \"core_model\": HoeffdingTreeRegressor,\n",
    "               \"model_name\": \"HoeffdingTreeRegressor\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that generates a list from a dictionary. It takes the values from the keys \"upper\" in the dictionary and returns a list of the values in the same order as the keys in the dictionary. For example if the dictionary is {\"a\": {\"upper\": 1}, \"b\": {\"upper\": 2}} the function should return the list [1, 2].\n",
    "def get_lower_values_from_dict(d):\n",
    "    lower = []\n",
    "    for key, value in d.items():\n",
    "        lower.append(value[\"lower\"])\n",
    "    return lower\n",
    "lower = get_lower_values_from_dict(fun_control[\"algorithm_hyper_dict\"])\n",
    "print(lower)\n",
    "def get_upper_values_from_dict(d):\n",
    "    upper = []\n",
    "    for key, value in d.items():\n",
    "        upper.append(value[\"upper\"])\n",
    "    return upper\n",
    "upper = get_upper_values_from_dict(fun_control[\"algorithm_hyper_dict\"])\n",
    "print(upper)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_values_from_dict(d):\n",
    "    new_list = []\n",
    "    for key, value in d.items():\n",
    "        new_list.append(value[\"transform\"])\n",
    "    return new_list\n",
    "t_val = get_transform_values_from_dict(fun_control[\"algorithm_hyper_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"]())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fun_control.update({\"data\": None, # dataset,\n",
    "               \"train\": train,\n",
    "               \"test\": test,\n",
    "               \"target_column\": target_column,\n",
    "               \"horizon\": horizon,\n",
    "               \"oml_grace_period\": oml_grace_period,\n",
    "               \"n_samples\": n_samples,\n",
    "               \"weights\": np.array([1, 1/1000, 1/1000])*10_000.0,\n",
    "               \"step\": 100,\n",
    "               \"log_level\": 50,\n",
    "               \"weight_coeff\": 1.0,\n",
    "               \"metric\": metrics.MAE(),\n",
    "               \"var_name\": var_name,\n",
    "               \"var_type\": var_type,\n",
    "               \"prep_model\": prep_model,\n",
    "               \"core_model\": HoeffdingTreeRegressor,\n",
    "               \"model_name\": \"HoeffdingTreeRegressor\",})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let lower = [10, 2, 1e-08, 0.01, 0, 0, 0.9, 0, 2, 0, 100, 0.01, 0, 100.0, 100000, 0, 0, 0] and upper = [1000, 20, 1e-06, 0.1, 2, 2, 0.99, 2, 10, 1, 500, 0.1, 1, 1000.0, 1000000, 1, 1, 0]. Find entries that are equal. The ouput should be a list where 0 represents that the values are not equal and 1 represents that are equal.\n",
    "def find_equal_in_lists(lower, upper):\n",
    "    equal = []\n",
    "    for i in range(len(lower)):\n",
    "        if lower[i] == upper[i]:\n",
    "            equal.append(1)\n",
    "        else:\n",
    "            equal.append(0)\n",
    "    return equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon = 7*24\n",
    "oml_grace_period = 2\n",
    "fun = HyperRiver(seed=123, log_level=50).fun_oml_horizon\n",
    "from spotPython.spot import spot\n",
    "from math import inf\n",
    "spot_htr = spot.Spot(fun=fun,\n",
    "                   lower = lower,\n",
    "                   upper = upper,\n",
    "                   fun_evals = inf,\n",
    "                   fun_repeats = 1,\n",
    "                   max_time = 5,\n",
    "                   noise = False,\n",
    "                   tolerance_x = np.sqrt(np.spacing(1)),\n",
    "                   var_type=var_type,\n",
    "                   var_name=var_name,\n",
    "                   infill_criterion = \"y\",\n",
    "                   n_points = 1,\n",
    "                   seed=123,\n",
    "                   log_level = 50,\n",
    "                   show_models= False,\n",
    "                   show_progress= True,\n",
    "                   fun_control = fun_control,\n",
    "                   design_control={\"init_size\": 20,\n",
    "                                   \"repeats\": 1},\n",
    "                   surrogate_control={\"noise\": True,\n",
    "                                      \"cod_type\": \"norm\",\n",
    "                                      \"min_theta\": -4,\n",
    "                                      \"max_theta\": 3,\n",
    "                                      \"n_theta\": len(var_name),\n",
    "                                      \"model_optimizer\": differential_evolution,\n",
    "                                      \"model_fun_evals\": 10_000,\n",
    "                                      \"log_level\": 50\n",
    "                                      })\n",
    "spot_htr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "horizon = 10\n",
    "oml_grace_period = 2\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, oml_grace_period, metric=metric)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a new X arrives as a numerical value from SPOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leaf_prediction_test_value = 0\n",
    "X = np.array([[fun_control[\"algorithm_hyper_dict\"][key][\"default\"] for key in fun_control[\"algorithm_hyper_dict\"].keys()]])\n",
    "# if entry in X is a \"string\" replace it with the numerical value zero (0)\n",
    "X = np.where(X == \"string\", 0, X)\n",
    "X.shape[1]\n",
    "print(\"X\", X)\n",
    "var_dict = assign_values(X, fun_control[\"var_name\"])\n",
    "for values in iterate_dict_values(var_dict):\n",
    "            values = convert_keys(values, fun_control[\"var_type\"])\n",
    "            print(fun_control[\"algorithm_hyper_dict\"])\n",
    "            print(\"values:\", values)\n",
    "            ## values = apply_selectors(values, fun_control[\"core_model\"].__name__, fun_control[\"algorithm_hyper_dict\"])\n",
    "            values = get_dict_with_levels_and_types(d=fun_control[\"algorithm_hyper_dict\"], v=values)\n",
    "            print(\"values:\", values)\n",
    "            ## model = river.compose.Pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"](**values))\n",
    "            model = Pipeline(fun_control[\"prep_model\"], fun_control[\"core_model\"]())\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_dict_with_levels(d=fun_control[\"algorithm_hyper_dict\"], v=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values = get_dict_with_levels_and_types(d=fun_control[\"algorithm_hyper_dict\"], v=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from river import linear_model\n",
    "from river import preprocessing, datasets\n",
    "from spotRiver.evaluation.eval_bml import eval_oml_horizon\n",
    "from river import datasets\n",
    "import pandas as pd\n",
    "from spotRiver.utils.data_conversion import convert_to_df\n",
    "dataset = datasets.TrumpApproval()\n",
    "target_column = \"Approval\"\n",
    "df = convert_to_df(dataset, target_column)\n",
    "df.rename(columns={'date': 'ordinal_date', 'Gallup': 'gallup', 'Ipsos': 'ipsos', 'Morning Consult': 'morning_consult', 'Rasmussen': 'rasmussen', 'YouGov': 'you_gov'}, inplace=True)\n",
    "train = df[0:499]\n",
    "test = df[500:-1]\n",
    "## model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "metric = mae\n",
    "horizon = 10\n",
    "oml_grace_period = 2\n",
    "df_eval, df_preds = eval_oml_horizon(model, train, test, target_column, horizon, oml_grace_period, metric=metric)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spotRiver.data.river_hyper_dict import HyperDict\n",
    "from spotPython.hyperparameters.categorical import one_hot_encode, sum_encoded_values, get_one_hot, add_missing_elements,find_closest_key\n",
    "from spotRiver.utils.assignments import assign_values, iterate_dict_values, convert_keys\n",
    "river_hyper_dict = HyperDict().load()\n",
    "fun_control = {}\n",
    "hyper_dict = {}\n",
    "algorithm = \"HoeffdingTreeRegressor\"\n",
    "hyperparameter = \"leaf_prediction\"\n",
    "# leaf_prediction_dict = get_one_hot(algorithm, hyperparameter,filename=\"./river_hyper_dict.json\")\n",
    "leaf_prediction_dict = get_one_hot(algorithm, hyperparameter, d=river_hyper_dict)\n",
    "print(\"leaf_prediction_dict:\", leaf_prediction_dict)\n",
    "hyper_dict[hyperparameter] = leaf_prediction_dict\n",
    "print(\"hyper_dict:\", hyper_dict)\n",
    "fun_control[\"hyper_dict\"] = hyper_dict\n",
    "print(\"fun_control:\", fun_control)\n",
    "lb = [\"mean\"]\n",
    "ub = [\"adaptive\"]\n",
    "add_missing_elements(lb, ub)\n",
    "print(\"lb:\", lb)\n",
    "print(\"ub:\", ub)\n",
    "a = sum_encoded_values(lb, fun_control[\"hyper_dict\"][\"leaf_prediction\"])\n",
    "b = sum_encoded_values(ub, fun_control[\"hyper_dict\"][\"leaf_prediction\"])\n",
    "leaf_prediction_lower = min(a,b)\n",
    "leaf_prediction_upper = max(a,b)\n",
    "print(\"leaf_prediction_lower:\", leaf_prediction_lower)\n",
    "print(\"leaf_prediction_upper:\", leaf_prediction_upper)\n",
    "#\n",
    "leaf_prediction_test_value = 4\n",
    "X = np.array([200,\n",
    "                  6,\n",
    "                  1e-6,\n",
    "                  0.075,\n",
    "                  leaf_prediction_test_value,\n",
    "                  -1,\n",
    "                  0.975,\n",
    "                  -1,\n",
    "                  10,\n",
    "                  1,\n",
    "                  750.0])\n",
    "fun_control.update({\"var_name\": [\"grace_period\",\n",
    "          \"max_depth\",\n",
    "          \"delta\",\n",
    "          \"tau\",\n",
    "          \"leaf_prediction\",\n",
    "          \"leaf_model\",\n",
    "          \"model_selector_decay\",\n",
    "          \"splitter\",\n",
    "          \"min_samples_split\",\n",
    "          \"binary_split\",\n",
    "          \"max_size\"],\n",
    "            \"var_type\": [\"int\",\n",
    "            \"int\",\n",
    "            \"num\",\n",
    "            \"num\",\n",
    "            \"factor\",\n",
    "            \"factor\",\n",
    "            \"num\",\n",
    "            \"factor\",\n",
    "            \"int\",\n",
    "            \"factor\",\n",
    "            \"num\"]})\n",
    "print(\"fun_control\", fun_control)\n",
    "X = np.array([X])\n",
    "leaf_prediction_test_value = 6\n",
    "X = np.array([[200, 10, 1e-07, 0.05, leaf_prediction_test_value, 0, 0.95, 1, 9, 0, 500.0]])\n",
    "X.shape[1]\n",
    "# print(\"X:\", X)\n",
    "# print(\"fun_control[var_name]:\",fun_control[\"var_name\"])\n",
    "var_dict = assign_values(X, fun_control[\"var_name\"])\n",
    "for values in iterate_dict_values(var_dict):\n",
    "            values = convert_keys(values, fun_control[\"var_type\"])\n",
    "            ## values = apply_selectors(values, fun_control[\"core_model\"].__name__, fun_control[\"hyper_dict\"])\n",
    "d = values\n",
    "print(d)\n",
    "hyper_dict = fun_control[\"hyper_dict\"]\n",
    "d[\"leaf_prediction\"] = find_closest_key(d[\"leaf_prediction\"], hyper_dict[\"leaf_prediction\"])\n",
    "print(d[\"leaf_prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests Hyperriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "\n",
    "def test_compute_y():\n",
    "    # create a Hyperriver object\n",
    "    hr = HyperRiver()\n",
    "\n",
    "    # create a sample evaluation DataFrame\n",
    "    df_eval = pd.DataFrame({\n",
    "        \"Metric\": [0.1, 0.2, 0.3],\n",
    "        \"CompTime (s)\": [1.0, 2.0, 3.0],\n",
    "        \"Memory (MB)\": [10.0, 20.0, 30.0]\n",
    "    })\n",
    "\n",
    "    # set the weights\n",
    "    hr.fun_control[\"weights\"] = [1, 2, 3]\n",
    "\n",
    "    # compute the objective function value\n",
    "    y = hr.compute_y(df_eval)\n",
    "\n",
    "    # check that the result is correct\n",
    "    assert y == 0.1*1 + 0.2*2 + 0.3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_compute_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spotRiver.fun.hyperriver import HyperRiver\n",
    "import pandas as pd\n",
    "hr = HyperRiver()\n",
    "df_eval = pd.DataFrame( [[1, 2, 3], [3, 4, 5]], columns=['Metric', 'CompTime (s)', 'Memory (MB)'])\n",
    "hr.compute_y(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
